
============================================================
========================= FILE: configs/infer_tracks.yaml =========================
============================================================

#configs/infer_tracks.yaml

openpose:
  root: "/home/bohdan/openpose"   # path to openpose root
  hand: false
  face: false
  net_resolution: "-1x256"
  num_gpu: 1
  num_gpu_start: 0
  render_pose: 0
  disable_blending: true
  number_people_max: 5
  disable_multi_thread: true

data:
  image_dir: "/home/bohdan/Документи/Samples/Images/boxing/Processed_data/output/"
  save_width: 800 # SAVE_WIDTH

tracking:
  config_path: "configs/tracking.yaml"
  num_frames_merge: 40           # how many pictures at one time will appear



============================================================
========================= FILE: configs/shot_boundary.yaml =========================
============================================================

shot_boundary:
  resize: [160, 90]
  grid: [4, 4]
  ema_alpha: 0.2
  cut_threshold: 0.45
  min_frames_between_cuts: 8


============================================================
========================= FILE: configs/tracking.yaml =========================
============================================================

tracking:
  show: 2                        # 0 = off, 1 = basic logs, 2 = full debug
  fps: 50.0                         # frames per seconds
  kalman:
    process_var: 68.0                # σ_a^2 — dispersion of accelerating
    measure_var: 58.0                # noise of detection
    p0: 1000.0                      # start covariation
  matching:
    alpha: 0.8                      # ration coordinates to pose
    chi2_gating: 80             # χ²(2, 0.99)
    pose_scale_eps: 1.0e-6          # min value to avoid dividing by zero
    large_cost: 880.0               # threshold to  C[i,j]
    min_kp_conf: 0.05               # min conf to joint
  tracker:
    max_age: 40                     # how long we can use track without update
    min_hits: 20                     # from what updating we can mark track as a reliable
    min_kp_conf: 0.05               # min conf to joint while update
    expect_body25: true             #  BODY_25 from OpenPose

============================================================
========================= FILE: configs/train_apperance_cnn.yaml =========================
============================================================

seed: 42

data:
  train_pairs: "artifacts/data/apperance/train_pairs.npz"
  val_pairs: "artifacts/data/apperance/val_pairs.npz"
  image_size: [128, 128]
  to_rgb: false   # true якщо твої кропи BGR з OpenCV

model:
  backbone: "mobilenetv3small"
  embedding_dim: 128
  dropout: 0.1
  l2_reg: 0.0
  train_backbone: true

training:
  batch_size: 64
  learning_rate: 0.0003
  epochs: 20
  margin: 1.0
  save_dir: "artifacts/models/apperance_cnn"
  save_name: "apperance_encoder.h5"


============================================================
========================= FILE: configs/train_pose_mpl.yaml =========================
============================================================

seed: 42

data:
  train_pairs: "artifacts/datasets/pose_pairs_train.npz"
  val_pairs: "artifacts/datasets/pose_pairs_val.npz"
  num_keypoints: 25

model:
  embedding_dim: 64
  hidden_dims: [256, 128]
  dropout: 0.1
  l2_reg: 1.0e-4

training:
  batch_size: 256
  epochs: 40
  learning_rate: 1.0e-3
  log_every: 50
  save_dir: "artifacts/models/pose_mlp"
  save_name: "pose_mlp_body25.h5"
  loss: "contrastive"   # або "triplet", якщо захочеш


============================================================
========================= FILE: scripts/infer_tracks.py =========================
============================================================

# scripts/infer_tracks.py


# (якщо є shebang або коментарі — вони можуть бути вище)

import sys
from pathlib import Path


ROOT = Path(__file__).resolve().parents[1]
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))



import yaml
from pathlib import Path
import sys

from src.boxing_project.tracking.inference_utils import (
    init_openpose_from_config,
    visualize_sequence,
)
from src.boxing_project.tracking.tracker import MultiObjectTracker


ROOT = Path(__file__).resolve().parents[1]
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

# !!! do NOT import src.boxing_project.utils.config here !!!

def load_config(path: Path):
    """Load YAML file into a Python dict, without any heavy side-effects."""
    with open(path, "r") as f:
        return yaml.safe_load(f)



def get_project_root() -> Path:
    """
    Resolve project root as the parent of the 'scripts' directory.
    This file is .../Boxing_Project/scripts/infer_tracks.py
    so project root is parents[1].
    """
    return Path(__file__).resolve().parents[1]


def main():
    project_root = get_project_root()

    # --- config path resolved from project root ---
    cfg_path = project_root / "configs" / "infer_tracks.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"Config file not found: {cfg_path}")

    cfg = load_config(cfg_path)

    # --- OpenPose ---
    _, opWrapper = init_openpose_from_config(cfg["openpose"])

    # --- tracker ---
    tracking_cfg = cfg["tracking"]
    tracking_cfg_path = tracking_cfg["config_path"]

    # if config_path is relative → resolve from project root
    tracking_cfg_path = (
        project_root / tracking_cfg_path
        if not Path(tracking_cfg_path).is_absolute()
        else Path(tracking_cfg_path)
    )

    if not tracking_cfg_path.exists():
        raise FileNotFoundError(f"Tracking config not found: {tracking_cfg_path}")

    tracker = MultiObjectTracker(config_path=str(tracking_cfg_path))

    # --- images ---
    data_cfg = cfg["data"]
    image_dir = Path(data_cfg["image_dir"])
    if not image_dir.is_absolute():
        image_dir = project_root / image_dir

    if not image_dir.exists():
        raise FileNotFoundError(f"Image directory does not exist: {image_dir}")

    images = sorted(
        p for p in image_dir.rglob("*")
        if p.suffix.lower() in (".jpg", ".jpeg", ".png")
    )



    if not images:
        raise RuntimeError(f"No images found in directory: {image_dir}")

    # --- run inference loop ---
    visualize_sequence(
        opWrapper=opWrapper,
        tracker=tracker,
        images=images,
        save_width=data_cfg["save_width"],
        merge_n=tracking_cfg["num_frames_merge"],
    )


if __name__ == "__main__":
    main()


============================================================
========================= FILE: scripts/train_apperance_cnn.py =========================
============================================================

from pathlib import Path
import yaml
import tensorflow as tf

from src.boxing_project.utils.config import set_seed
from src.boxing_project.apperance_embedding.dataset import CropPairs, prepare_contrastive_arrays
from src.boxing_project.apperance_embedding.cnn_model import AppearanceCNNConfig, build_appearance_cnn
from src.boxing_project.apperance_embedding.losses import contrastive_loss


def load_yaml(path: Path) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)


def main():
    project_root = Path(__file__).resolve().parents[1]
    cfg = load_yaml(project_root / "configs" / "train_apperance_cnn.yaml")

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    # ---------- DATA ----------
    data_cfg = cfg["data"]
    train_pairs = CropPairs.from_npz(str(project_root / data_cfg["train_pairs"]))
    val_pairs   = CropPairs.from_npz(str(project_root / data_cfg["val_pairs"]))

    image_size = tuple(data_cfg.get("image_size", [128, 128]))
    to_rgb = bool(data_cfg.get("to_rgb", False))

    X1_tr, X2_tr, y_tr = prepare_contrastive_arrays(train_pairs, image_size=image_size, to_rgb=to_rgb)
    X1_va, X2_va, y_va = prepare_contrastive_arrays(val_pairs,   image_size=image_size, to_rgb=to_rgb)

    batch_size = int(cfg["training"]["batch_size"])

    train_ds = (
        tf.data.Dataset.from_tensor_slices(((X1_tr, X2_tr), y_tr))
        .shuffle(len(y_tr), seed=seed, reshuffle_each_iteration=True)
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    val_ds = (
        tf.data.Dataset.from_tensor_slices(((X1_va, X2_va), y_va))
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    # ---------- MODEL ----------
    model_cfg = cfg["model"]
    cnn_cfg = AppearanceCNNConfig(
        image_size=image_size,
        embedding_dim=int(model_cfg.get("embedding_dim", 128)),
        backbone=str(model_cfg.get("backbone", "mobilenetv3small")),
        dropout=float(model_cfg.get("dropout", 0.0)),
        l2_reg=float(model_cfg.get("l2_reg", 0.0)),
        train_backbone=bool(model_cfg.get("train_backbone", True)),
    )

    encoder = build_appearance_cnn(cnn_cfg)

    # Siamese training wrapper: two crops -> distance
    in_a = tf.keras.Input(shape=(image_size[0], image_size[1], 3), name="crop_a")
    in_b = tf.keras.Input(shape=(image_size[0], image_size[1], 3), name="crop_b")

    e_a = encoder(in_a)
    e_b = encoder(in_b)

    distance = tf.norm(e_a - e_b, axis=-1, keepdims=True)  # (B,1)

    siamese = tf.keras.Model([in_a, in_b], distance, name="appearance_siamese")

    # ---------- TRAIN ----------
    lr = float(cfg["training"]["learning_rate"])
    margin = float(cfg["training"].get("margin", 1.0))

    siamese.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss=contrastive_loss(margin=margin),
    )

    siamese.fit(
        train_ds,
        validation_data=val_ds,
        epochs=int(cfg["training"]["epochs"]),
    )

    # ---------- SAVE ----------
    save_cfg = cfg["training"]
    save_dir = project_root / save_cfg.get("save_dir", "artifacts/models/apperance_cnn")
    save_dir.mkdir(parents=True, exist_ok=True)

    save_path = save_dir / save_cfg.get("save_name", "apperance_encoder.h5")

    encoder.save(save_path)
    print(f"\nSaved appearance encoder to: {save_path}")


if __name__ == "__main__":
    main()


============================================================
========================= FILE: scripts/train_pose_mpl.py =========================
============================================================

# scripts/train_pose_mlp.py

from pathlib import Path
import yaml
import tensorflow as tf

from src.boxing_project.utils.config import set_seed
from src.boxing_project.pose_embeding.mlp_model import PoseMLPConfig, build_pose_mlp
from src.boxing_project.pose_embeding.dataset import PosePairs, prepare_contrastive_arrays
from src.boxing_project.pose_embeding.losses import contrastive_loss


def load_yaml(path: Path) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)


def main():
    project_root = Path(__file__).resolve().parents[1]

    # ---------- CONFIG ----------
    cfg = load_yaml(project_root / "configs" / "train_pose_mlp.yaml")

    seed = int(cfg.get("seed", 42))
    set_seed(seed)

    # ---------- DATA ----------
    data_cfg = cfg["data"]

    train_pairs = PosePairs.from_npz(project_root / data_cfg["train_pairs"])
    val_pairs   = PosePairs.from_npz(project_root / data_cfg["val_pairs"])

    num_keypoints = int(data_cfg["num_keypoints"])
    batch_size = int(cfg["training"]["batch_size"])

    X1_tr, X2_tr, y_tr = prepare_contrastive_arrays(train_pairs)
    X1_va, X2_va, y_va = prepare_contrastive_arrays(val_pairs)

    train_ds = (
        tf.data.Dataset.from_tensor_slices(((X1_tr, X2_tr), y_tr))
        .shuffle(len(y_tr), seed=seed, reshuffle_each_iteration=True)
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    val_ds = (
        tf.data.Dataset.from_tensor_slices(((X1_va, X2_va), y_va))
        .batch(batch_size)
        .prefetch(tf.data.AUTOTUNE)
    )

    # ---------- MODEL ----------
    model_cfg = cfg["model"]

    pose_cfg = PoseMLPConfig(
        num_keypoints=num_keypoints,
        embedding_dim=int(model_cfg["embedding_dim"]),
        hidden_dims=list(model_cfg["hidden_dims"]),
        dropout=float(model_cfg.get("dropout", 0.0)),
        l2_reg=float(model_cfg.get("l2_reg", 0.0)),
    )

    # encoder (ТЕ, ЩО НАМ ПОТРІБНО В ПРОДАКШЕНІ)
    base_mlp = build_pose_mlp(pose_cfg)

    # siamese training wrapper
    input_a = tf.keras.Input(shape=(num_keypoints * 2,), name="pose_a")
    input_b = tf.keras.Input(shape=(num_keypoints * 2,), name="pose_b")

    emb_a = base_mlp(input_a)
    emb_b = base_mlp(input_b)

    # ❗ МОДЕЛЬ ПОВЕРТАЄ DISTANCE
    distance = tf.norm(emb_a - emb_b, axis=-1, keepdims=True)

    siamese_model = tf.keras.Model(
        inputs=[input_a, input_b],
        outputs=distance,
        name="pose_siamese"
    )

    # ---------- TRAIN ----------
    siamese_model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=float(cfg["training"]["learning_rate"])
        ),
        loss=contrastive_loss(margin=1.0),
    )

    siamese_model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=int(cfg["training"]["epochs"]),
    )


    # ---------- SAVE ----------
    save_cfg = cfg["training"]
    save_dir = project_root / save_cfg.get("save_dir", "artifacts/models/pose_mlp")
    save_dir.mkdir(parents=True, exist_ok=True)

    save_path = save_dir / save_cfg.get("save_name", "pose_mlp_body25.h5")

    # ❗ ЗБЕРІГАЄМО ТІЛЬКИ ENCODER
    base_mlp.save(save_path)
    print(f"\nSaved pose encoder to: {save_path}")


if __name__ == "__main__":
    main()


============================================================
========================= FILE: src/boxing_project/apperance_embedding/cnn_model.py =========================
============================================================

from dataclasses import dataclass
import tensorflow as tf


@dataclass
class AppearanceCNNConfig:
    image_size: tuple[int, int] = (128, 128)
    embedding_dim: int = 128
    backbone: str = "mobilenetv3small"  # future-proof
    dropout: float = 0.0
    l2_reg: float = 0.0
    train_backbone: bool = True  # can freeze for warmup if you want


def build_appearance_cnn(cfg: AppearanceCNNConfig) -> tf.keras.Model:
    """
    Returns encoder model:
      (H,W,3) -> (embedding_dim,)
    Embeddings are L2-normalized => cosine distance works well.
    """
    h, w = cfg.image_size
    inputs = tf.keras.Input(shape=(h, w, 3), name="crop")

    reg = tf.keras.regularizers.l2(cfg.l2_reg) if cfg.l2_reg > 0 else None

    if cfg.backbone.lower() == "mobilenetv3small":
        base = tf.keras.applications.MobileNetV3Small(
            include_top=False,
            weights="imagenet",
            input_tensor=inputs,
        )
    else:
        raise ValueError(f"Unknown backbone: {cfg.backbone}")

    base.trainable = bool(cfg.train_backbone)

    x = base.output
    x = tf.keras.layers.GlobalAveragePooling2D(name="gap")(x)

    if cfg.dropout > 0:
        x = tf.keras.layers.Dropout(cfg.dropout)(x)

    x = tf.keras.layers.Dense(cfg.embedding_dim, kernel_regularizer=reg, name="proj")(x)

    # L2 normalize embedding
    x = tf.keras.layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=-1), name="l2norm")(x)

    return tf.keras.Model(inputs, x, name="appearance_encoder")


============================================================
========================= FILE: src/boxing_project/apperance_embedding/dataset.py =========================
============================================================

from dataclasses import dataclass
import numpy as np

from .preprocessing import preprocess_crops_np


@dataclass
class CropPairs:
    """
    img_a: (N, H, W, 3) uint8/float
    img_b: (N, H, W, 3) uint8/float
    labels: (N,) 0/1
    """
    img_a: np.ndarray
    img_b: np.ndarray
    labels: np.ndarray

    @classmethod
    def from_npz(cls, path: str) -> "CropPairs":
        data = np.load(path)
        return cls(
            img_a=data["img_a"],
            img_b=data["img_b"],
            labels=data["label"],
        )


def prepare_contrastive_arrays(
    pairs: CropPairs,
    image_size: tuple[int, int] = (128, 128),
    to_rgb: bool = False,
) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Returns:
      X1: (N, H, W, 3) float32 [0,1]
      X2: (N, H, W, 3) float32 [0,1]
      y:  (N,) float32
    """
    X1 = preprocess_crops_np(pairs.img_a, image_size=image_size, to_rgb=to_rgb)
    X2 = preprocess_crops_np(pairs.img_b, image_size=image_size, to_rgb=to_rgb)
    y = pairs.labels.astype(np.float32)
    return X1, X2, y


============================================================
========================= FILE: src/boxing_project/apperance_embedding/inference.py =========================
============================================================

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Tuple

import numpy as np

try:
    import tensorflow as tf
except Exception:
    tf = None

from .preprocessing import preprocess_crops_np


@dataclass(frozen=True)
class AppearanceEmbedConfig:
    """Конфіг для AppearanceEmbedder (шлях до моделі + препроцес)."""
    model_path: str
    to_rgb: bool = True
    l2_normalize: bool = True


class AppearanceEmbedder:
    """Класовий інференс appearance-embedding: вантажимо CNN 1 раз, далі embed() для кожного bbox."""

    def __init__(self, cfg: AppearanceEmbedConfig):
        """Завантажує appearance encoder з cfg.model_path (artifacts) та тримає його в памʼяті."""
        if tf is None:
            raise RuntimeError("TensorFlow не доступний. Перевір встановлення tensorflow у venv.")
        self.cfg = cfg
        self.model = tf.keras.models.load_model(str(Path(cfg.model_path)), compile=False)

    def embed(self, frame_bgr: np.ndarray, bbox_xyxy: Tuple[int, int, int, int]) -> np.ndarray:
        """
        Рахує appearance embedding для одного bbox.

        Args:
            frame_bgr: кадр OpenCV (H,W,3) BGR.
            bbox_xyxy: (x1,y1,x2,y2) int.

        Returns:
            np.ndarray (D,) — embedding (опційно L2-normalized).
        """
        x1, y1, x2, y2 = bbox_xyxy
        h, w = frame_bgr.shape[:2]

        x1 = int(max(0, min(w - 1, x1)))
        x2 = int(max(0, min(w, x2)))
        y1 = int(max(0, min(h - 1, y1)))
        y2 = int(max(0, min(h, y2)))

        if x2 <= x1 or y2 <= y1:
            # Якщо bbox вироджений — повернемо нульовий embedding (щоб не падати)
            return np.zeros((128,), dtype=np.float32)

        crop = frame_bgr[y1:y2, x1:x2]
        crops = np.expand_dims(crop, axis=0)  # (1,h,w,3)

        x = preprocess_crops_np(crops, to_rgb=self.cfg.to_rgb)
        emb = self.model.predict(x, verbose=0)[0].astype(np.float32)

        if self.cfg.l2_normalize:
            n = float(np.linalg.norm(emb) + 1e-12)
            emb = emb / n

        return emb


============================================================
========================= FILE: src/boxing_project/apperance_embedding/__init__.py =========================
============================================================

"""
Apperance embedding module.

Goal:
  crop (bbox RGB/BGR image) -> embedding vector (D,)
Training:
  siamese wrapper + contrastive loss on pairs of crops (same/different boxer).
Usage (in inference/tracking):
  e_app = encoder(preprocess(crop))
"""

from .cnn_model import AppearanceCNNConfig, build_appearance_cnn
from .dataset import CropPairs, prepare_contrastive_arrays
from .losses import contrastive_loss


============================================================
========================= FILE: src/boxing_project/apperance_embedding/losses.py =========================
============================================================

import tensorflow as tf


def contrastive_loss(margin: float = 1.0):
    """
    Contrastive loss:
      y=1 -> positive -> minimize distance
      y=0 -> negative -> push distance >= margin

    y_true: (B,) float 0/1
    y_pred: (B,1) distance
    """
    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        d = tf.squeeze(y_pred, axis=-1)  # (B,)

        pos = y_true * tf.square(d)
        neg = (1.0 - y_true) * tf.square(tf.maximum(margin - d, 0.0))
        return tf.reduce_mean(pos + neg)

    return loss


============================================================
========================= FILE: src/boxing_project/apperance_embedding/preprocessing.py =========================
============================================================

import numpy as np
import tensorflow as tf


def preprocess_crops_np(
    imgs: np.ndarray,
    image_size: tuple[int, int] = (128, 128),
    to_rgb: bool = False,
) -> np.ndarray:
    """
    imgs: (N, H, W, 3) uint8 or float32
    Returns: (N, image_size[0], image_size[1], 3) float32 in [0,1]

    to_rgb:
      - if your crops are BGR (from OpenCV), set to_rgb=True to swap channels.
      - if your crops are already RGB, keep False.
    """
    if imgs.dtype != np.float32:
        imgs = imgs.astype(np.float32)

    if imgs.max() > 1.5:  # assume uint8 in [0..255]
        imgs = imgs / 255.0

    if to_rgb:
        imgs = imgs[..., ::-1]  # BGR -> RGB

    # resize with TF (fast, good quality)
    x = tf.convert_to_tensor(imgs, dtype=tf.float32)
    x = tf.image.resize(x, image_size, method="bilinear", antialias=True)
    x = tf.clip_by_value(x, 0.0, 1.0)
    return x.numpy()


============================================================
========================= FILE: src/boxing_project/__init__.py =========================
============================================================



============================================================
========================= FILE: src/boxing_project/kalman_filter/__init__.py =========================
============================================================



============================================================
========================= FILE: src/boxing_project/kalman_filter/kalman.py =========================
============================================================

# src/boxing_project/kalman_filter/kalman.py

import numpy as np
from filterpy.kalman import KalmanFilter
from typing import Union, Tuple


def _q_block(dt: float, var: float) -> np.ndarray:
    """1D constant-acceleration Q-блок для [pos, vel]."""
    dt2 = dt * dt
    dt3 = dt2 * dt
    dt4 = dt3 * dt
    return var * np.array(
        [
            [dt4 / 4.0, dt3 / 2.0],
            [dt3 / 2.0, dt2],
        ],
        dtype=float,
    )


def _ensure_state(x0: Union[np.ndarray, list, tuple]) -> np.ndarray:
    """
    Приводимо початковий стан до (4,1): [x, y, vx, vy]^T.

    Дозволено:
      - [x, y]
      - [x, y, vx, vy]
    """
    x0 = np.asarray(x0, dtype=float).reshape(-1)
    if x0.size == 2:
        x0 = np.array([x0[0], x0[1], 0.0, 0.0], dtype=float)
    elif x0.size != 4:
        raise ValueError("x0 must have length 2 ([x,y]) or 4 ([x,y,vx,vy])")
    return x0.reshape(4, 1)


def _ensure_measurement(z: Union[np.ndarray, list, tuple]) -> np.ndarray:
    """Приводимо вимір до (2,1): [x, y]^T."""
    z = np.asarray(z, dtype=float).reshape(-1)
    if z.size != 2:
        raise ValueError("Measurement z must have length 2: [x, y]")
    return z.reshape(2, 1)


class KalmanTracker:
    """
    Проста 2D Kalman-модель з постійною швидкістю.

    Стан:        [x, y, vx, vy]^T
    Вимір:       [x, y]^T
    """

    def __init__(
        self,
        x0: Union[np.ndarray, list, tuple],
        dt: float,
        process_var: float,
        measure_var: float,
        p0: float,
    ):
        # нормалізуємо початковий стан
        x0 = _ensure_state(x0)
        self.dt = float(dt)

        # внутрішній KalmanFilter з filterpy
        self.kf = KalmanFilter(dim_x=4, dim_z=2)

        # Матриця переходу стану (constant velocity)
        self.kf.F = np.array(
            [
                [1.0, 0.0, self.dt, 0.0],
                [0.0, 1.0, 0.0, self.dt],
                [0.0, 0.0, 1.0, 0.0],
                [0.0, 0.0, 0.0, 1.0],
            ],
            dtype=float,
        )

        # Вимірюємо тільки позицію (x, y)
        self.kf.H = np.array(
            [
                [1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0],
            ],
            dtype=float,
        )

        # Коваріація вимірювального шуму
        self.kf.R = float(measure_var) * np.eye(2, dtype=float)

        # Процесний шум (constant acceleration у x,y)
        Q_cv = np.block(
            [
                [_q_block(self.dt, process_var), np.zeros((2, 2))],
                [np.zeros((2, 2)), _q_block(self.dt, process_var)],
            ]
        )

        # Перестановка під порядок стану [x, y, vx, vy]
        Pperm = np.array(
            [
                [1, 0, 0, 0],  # x
                [0, 0, 1, 0],  # y
                [0, 1, 0, 0],  # vx
                [0, 0, 0, 1],  # vy
            ],
            dtype=float,
        )
        self.kf.Q = Pperm @ Q_cv @ Pperm.T

        # Початковий стан і коваріація
        self.kf.x = x0
        self.kf.P = np.eye(4, dtype=float) * float(p0)

    # -------- основний API --------

    def predict(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Прогноз на один крок.

        Повертає:
          state: (4,) [x, y, vx, vy]
          cov:   (4,4) P
        """
        self.kf.predict()
        return self.get_state(), self.get_cov()

    def update(self, z: Union[np.ndarray, list, tuple]) -> Tuple[np.ndarray, np.ndarray]:
        """
        Оновлення за новим виміром z = [x, y].

        Повертає:
          state: (4,) оновлений стан
          cov:   (4,4) P
        """
        z_norm = _ensure_measurement(z)
        self.kf.update(z_norm)
        return self.get_state(), self.get_cov()

    # -------- допоміжні методи --------

    def project(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Проєкція поточного стану в простір вимірювань.

        Повертає:
          z_hat: (2,1) прогнозований вимір
          S:     (2,2) коваріація інновації
        """
        H = self.kf.H
        x = self.kf.x
        P = self.kf.P
        z_hat = H @ x
        S = H @ P @ H.T + self.kf.R
        return z_hat, S

    def gating_distance(self, z: Union[np.ndarray, list, tuple]) -> float:
        """
        Махаланобісова відстань^2 між виміром z та прогнозом.
        Використовується для χ²-gating.
        """
        z = _ensure_measurement(z)
        z_hat, S = self.project()
        r = z - z_hat  # інновація

        try:
            S_inv = np.linalg.inv(S)
        except np.linalg.LinAlgError:
            S_inv = np.linalg.pinv(S + 1e-6 * np.eye(2))

        d2 = float(r.T @ S_inv @ r)
        return d2

    def get_state(self) -> np.ndarray:
        """Поточний стан як (4,) [x, y, vx, vy]."""
        return self.kf.x.reshape(-1).copy()

    def get_cov(self) -> np.ndarray:
        """Поточна коваріація (4x4)."""
        return self.kf.P.copy()

    @property
    def F(self) -> np.ndarray:
        return self.kf.F

    @property
    def Q(self) -> np.ndarray:
        return self.kf.Q

    @property
    def R(self) -> np.ndarray:
        return self.kf.R

    @property
    def H(self) -> np.ndarray:
        return self.kf.H


============================================================
========================= FILE: src/boxing_project/pose_embeding/dataset.py =========================
============================================================

# src/boxing_project/pose_embeding/dataset.py

from dataclasses import dataclass
import numpy as np
from .normalization import normalize_pose_2d


@dataclass
class PosePairs:
    pose_a: np.ndarray  # (N, K, 3) or (N, K, 2)
    pose_b: np.ndarray  # (N, K, 3) or (N, K, 2)
    labels: np.ndarray  # (N,)

    @classmethod
    def from_npz(cls, path: str) -> "PosePairs":
        data = np.load(path)
        return cls(
            pose_a=data["pose_a"],
            pose_b=data["pose_b"],
            labels=data["label"],
        )


def normalize_and_flatten_all(pose: np.ndarray) -> np.ndarray:
    """
    pose: (N, K, 2 or 3)
    return: (N, 2*K) float32
    """
    N, K, D = pose.shape
    out = np.zeros((N, K * 2), dtype=np.float32)

    for i in range(N):
        kp = pose[i]
        norm_kp, _, _ = normalize_pose_2d(kp)   # expected (K,2)
        out[i] = norm_kp.reshape(-1)

    return out


def prepare_contrastive_arrays(pairs: PosePairs) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Returns:
      X1: (N, 2K)
      X2: (N, 2K)
      y:  (N,) float32
    """
    X1 = normalize_and_flatten_all(pairs.pose_a)
    X2 = normalize_and_flatten_all(pairs.pose_b)
    y = pairs.labels.astype(np.float32)
    return X1, X2, y


============================================================
========================= FILE: src/boxing_project/pose_embeding/inference.py =========================
============================================================

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import numpy as np

try:
    import tensorflow as tf
except Exception:
    tf = None

# IMPORTANT:
# Якщо у тебе інша назва функції нормалізації - зміни імпорт на свою.
from .normalization import normalize_keypoints


@dataclass(frozen=True)
class PoseEmbedConfig:
    """Конфіг для PoseEmbedder (шлях до моделі + дрібні параметри інференсу)."""
    model_path: str
    fill_nan_value: float = 0.0
    l2_normalize: bool = True


class PoseEmbedder:
    """Класовий інференс pose-embedding: модель вантажиться 1 раз, embed() викликається багато разів."""

    def __init__(self, cfg: PoseEmbedConfig):
        """Завантажує pose encoder з cfg.model_path (artifacts) та тримає його в памʼяті."""
        if tf is None:
            raise RuntimeError("TensorFlow не доступний. Перевір встановлення tensorflow у venv.")
        self.cfg = cfg
        self.model = tf.keras.models.load_model(str(Path(cfg.model_path)), compile=False)

    def embed(self, keypoints_xy: np.ndarray, kp_conf: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Рахує pose embedding для однієї людини.

        Args:
            keypoints_xy: np.ndarray форми (K,2) з координатами keypoints.
            kp_conf: optional np.ndarray форми (K,) з confidence. Якщо дано, точки з conf<=0 позначимо NaN.

        Returns:
            np.ndarray форми (D,) — embedding (опційно L2-normalized).
        """
        kps = np.asarray(keypoints_xy, dtype=np.float32)
        if kps.ndim != 2 or kps.shape[1] != 2:
            raise ValueError(f"Очікую keypoints форми (K,2), отримав {kps.shape}")

        if kp_conf is not None:
            conf = np.asarray(kp_conf, dtype=np.float32).reshape(-1)
            if conf.shape[0] != kps.shape[0]:
                raise ValueError(f"kp_conf має мати довжину K={kps.shape[0]}, отримав {conf.shape[0]}")
            bad = conf <= 0.0
            kps = kps.copy()
            kps[bad] = np.nan

        # Нормалізація скелета (центр/масштаб/поворот) — твоя функція
        kps_norm = normalize_keypoints(kps)

        # Замінюємо NaN на fill_nan_value, щоб модель могла порахувати embedding
        kps_norm = np.where(np.isfinite(kps_norm), kps_norm, self.cfg.fill_nan_value).astype(np.float32)

        flat = kps_norm.reshape(1, -1)  # (1, 2K)
        emb = self.model.predict(flat, verbose=0)[0].astype(np.float32)

        if self.cfg.l2_normalize:
            n = float(np.linalg.norm(emb) + 1e-12)
            emb = emb / n

        return emb


============================================================
========================= FILE: src/boxing_project/pose_embeding/__init__.py =========================
============================================================



============================================================
========================= FILE: src/boxing_project/pose_embeding/losses.py =========================
============================================================

# src/boxing_project/pose_embeding/losses.py

import tensorflow as tf


def contrastive_loss(margin: float = 1.0):
    """
    Contrastive loss for metric learning.

    y_true: (B,)    -> 1.0 positive, 0.0 negative
    y_pred: (B, 1)  -> distance between embeddings
    """

    def loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        d = tf.squeeze(y_pred, axis=-1)  # (B,)

        pos_loss = y_true * tf.square(d)
        neg_loss = (1.0 - y_true) * tf.square(tf.maximum(margin - d, 0.0))

        return tf.reduce_mean(pos_loss + neg_loss)

    return loss


============================================================
========================= FILE: src/boxing_project/pose_embeding/mpl_model.py =========================
============================================================

# src/boxing_project/pose_embeding/mlp_model.py

from dataclasses import dataclass
from typing import List
import tensorflow as tf
from tensorflow.keras import layers, regularizers, Model


@dataclass
class PoseMLPConfig:
    num_keypoints: int
    embedding_dim: int = 64
    hidden_dims: List[int] = None
    dropout: float = 0.0
    l2_reg: float = 0.0

    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [256, 128]


def build_pose_mlp(config: PoseMLPConfig) -> Model:
    """
    Build a simple MLP that maps flattened normalized pose (2*K) -> embedding_dim.
    """
    input_dim = config.num_keypoints * 2

    inputs = layers.Input(shape=(input_dim,), name="pose_input")

    x = inputs
    for i, h in enumerate(config.hidden_dims):
        x = layers.Dense(
            h,
            activation="relu",
            kernel_regularizer=regularizers.l2(config.l2_reg),
            name=f"dense_{i}"
        )(x)
        if config.dropout > 0.0:
            x = layers.Dropout(config.dropout, name=f"dropout_{i}")(x)

    # Final embedding
    x = layers.Dense(
        config.embedding_dim,
        activation=None,
        kernel_regularizer=regularizers.l2(config.l2_reg),
        name="embedding"
    )(x)

    # L2-normalize to use cosine similarity nicely
    outputs = tf.nn.l2_normalize(x, axis=-1, name="l2_normalized_embedding")

    model = Model(inputs=inputs, outputs=outputs, name="pose_mlp")
    return model


============================================================
========================= FILE: src/boxing_project/pose_embeding/normalization.py =========================
============================================================

import numpy as np

# Indexes for joints in OpenPose BODY_25
BODY_25 = {
    "Nose": 0,
    "Neck": 1,
    "RShoulder": 2,
    "RElbow": 3,
    "RWrist": 4,
    "LShoulder": 5,
    "LElbow": 6,
    "LWrist": 7,
    "MidHip": 8,       # root
    "RHip": 9,
    "RKnee": 10,
    "RAnkle": 11,
    "LHip": 12,
    "LKnee": 13,
    "LAnkle": 14,
    "REye": 15,
    "LEye": 16,
    "REar": 17,
    "LEar": 18,
    "LBigToe": 19,
    "LSmallToe": 20,
    "LHeel": 21,
    "RBigToe": 22,
    "RSmallToe": 23,
    "RHeel": 24
}


def center_skeleton_2d(keypoints, root_index):
    """
    Center the skeleton around the root joint.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2) or (N, 3)
        Keypoints for every joint. If shape is (N, 3), it is assumed [x, y, conf].
    root_index : int
        Index of the root joint (e.g. BODY_25["MidHip"]).

    Returns
    -------
    centered_keypoints : np.ndarray, shape (N, 2)
        Keypoints with root at (0, 0).
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)

    # Use only x, y
    if keypoints.shape[1] >= 2:
        keypoints_xy = keypoints[:, :2]
    else:
        raise ValueError("keypoints must have at least 2 columns (x, y).")

    root = keypoints_xy[root_index]  # (2,)
    centered = keypoints_xy - root   # (N, 2)
    return centered


def compute_scale_from_shoulders(keypoints, left_shoulder_idx, right_shoulder_idx, min_scale=1e-6):
    """
    Compute a scale factor based on the distance between shoulders.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2)
        Centered keypoints (x, y) for each joint.
    left_shoulder_idx : int
        Index of the left shoulder joint.
    right_shoulder_idx : int
        Index of the right shoulder joint.
    min_scale : float
        Minimum scale value to avoid division by zero.

    Returns
    -------
    scale : float
        Distance between shoulders (at least min_scale).
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)

    ls = keypoints[left_shoulder_idx]
    rs = keypoints[right_shoulder_idx]
    shoulder_vec = rs - ls
    dist = np.linalg.norm(shoulder_vec)

    if dist < min_scale:
        dist = min_scale
    return dist


def scale_skeleton_2d(keypoints, scale):
    """
    Scale the skeleton by dividing all coordinates by 'scale'.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2)
        Centered keypoints.
    scale : float
        Positive scale factor.

    Returns
    -------
    scaled_keypoints : np.ndarray, shape (N, 2)
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)
    return keypoints / float(scale)


def compute_shoulder_angle(keypoints, left_shoulder_idx, right_shoulder_idx):
    """
    Compute the angle (in radians) between the x-axis and the vector
    from left shoulder to right shoulder.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2)
        Centered & scaled keypoints.
    left_shoulder_idx : int
        Index of the left shoulder joint.
    right_shoulder_idx : int
        Index of the right shoulder joint.

    Returns
    -------
    angle : float
        Angle in radians.
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)

    ls = keypoints[left_shoulder_idx]
    rs = keypoints[right_shoulder_idx]
    v = rs - ls  # vector from left shoulder to right shoulder
    angle = np.arctan2(v[1], v[0])  # atan2(y, x)
    return angle


def rotate_skeleton_2d(keypoints, angle):
    """
    Rotate all keypoints by -angle to make the shoulder line horizontal.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2)
        Centered & scaled keypoints.
    angle : float
        Angle in radians that we want to remove.

    Returns
    -------
    rotated_keypoints : np.ndarray, shape (N, 2)
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)

    c = np.cos(-angle)
    s = np.sin(-angle)
    R = np.array([[c, -s],
                  [s,  c]], dtype=np.float32)  # 2x2 rotation matrix
    rotated = keypoints @ R.T  # (N, 2) @ (2, 2) -> (N, 2)
    return rotated


def normalize_pose_2d(
        keypoints,
        root_index=BODY_25["MidHip"],
        left_shoulder_idx=BODY_25["LShoulder"],
        right_shoulder_idx=BODY_25["RShoulder"],
        eps=1e-6
):
    """
    Full 2D pose normalization pipeline:
      1) Center around root (MidHip).
      2) Normalize scale based on shoulder distance.
      3) Rotate so that the shoulder line becomes horizontal.

    Parameters
    ----------
    keypoints : np.ndarray, shape (N, 2) or (N, 3)
        Original keypoints for BODY_25 joints.
    root_index : int
        Root joint index (default: MidHip).
    left_shoulder_idx : int
        Left shoulder index.
    right_shoulder_idx : int
        Right shoulder index.
    eps : float
        Small epsilon to avoid division by zero.

    Returns
    -------
    norm_keypoints : np.ndarray, shape (N, 2)
        Normalized skeleton (centered, scaled, rotated).
    scale : float
        Scale factor that was used.
    angle : float
        Original shoulder angle before rotation.
    """
    keypoints = np.asarray(keypoints, dtype=np.float32)

    # 1. Centering
    centered = center_skeleton_2d(keypoints, root_index)

    # 2. Scale
    scale = compute_scale_from_shoulders(centered, left_shoulder_idx, right_shoulder_idx, min_scale=eps)
    scaled = scale_skeleton_2d(centered, scale)

    # 3. Shoulder angle
    angle = compute_shoulder_angle(scaled, left_shoulder_idx, right_shoulder_idx)

    # 4. Rotate by -angle
    rotated = rotate_skeleton_2d(scaled, angle)

    return rotated, scale, angle


============================================================
========================= FILE: src/boxing_project/shout_boundary/detector.py =========================
============================================================

from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
from .ssim_detector import global_ssim
import cv2


@dataclass
class ShotBoundaryConfig:
    resize: Optional[Tuple[int, int]] = (160, 90)  # (width, height) or None
    grid: Tuple[int, int] = (4, 4)                 # (rows, cols)
    ema_alpha: float = 0.2                         # smoothing for g


class ShotBoundaryDetector:
    """
    Returns a single coefficient g in [0,1]:
      g ~ 1 => frames similar (same camera/shot)
      g ~ 0 => frames different (likely cut)
    """

    def __init__(self, cfg: ShotBoundaryConfig):
        self.cfg = cfg
        self.prev_gray: Optional[np.ndarray] = None
        self._g_ema: Optional[float] = None

    def reset(self):
        self.prev_gray = None
        self._g_ema = None

    def update(self, frame: np.ndarray) -> float:
        gray = self._prep_gray(frame)

        # first frame: no comparison possible
        if self.prev_gray is None:
            self.prev_gray = gray
            self._g_ema = 1.0
            return 1.0

        ssim_mean = self._blockwise_ssim_mean(self.prev_gray, gray)  # in ~[0,1]
        g_raw = float(np.clip(ssim_mean, 0.0, 1.0))

        # EMA smoothing on g
        if self._g_ema is None:
            g = g_raw
        else:
            a = float(self.cfg.ema_alpha)
            g = (1.0 - a) * float(self._g_ema) + a * g_raw

        self._g_ema = float(g)
        self.prev_gray = gray
        return float(g)

    # ---------- helpers ----------

    def _prep_gray(self, frame):
        x = frame.astype(np.float32)
        if x.max() > 1.5:
            x /= 255.0

        if x.ndim == 3:
            gray = 0.299 * x[..., 2] + 0.587 * x[..., 1] + 0.114 * x[..., 0]  # BGR → gray
        else:
            gray = x

        if self.cfg.resize is not None:
            w, h = self.cfg.resize
            gray = cv2.resize(gray, (w, h), interpolation=cv2.INTER_LINEAR)

        return np.clip(gray, 0.0, 1.0).astype(np.float32)

    def _blockwise_ssim_mean(self, prev_gray: np.ndarray, curr_gray: np.ndarray) -> float:
        rows, cols = self.cfg.grid
        H, W = prev_gray.shape

        row_edges = np.linspace(0, H, rows + 1, dtype=int)
        col_edges = np.linspace(0, W, cols + 1, dtype=int)

        s = 0.0
        n = 0

        for r in range(rows):
            rs, re = row_edges[r], row_edges[r + 1]
            for c in range(cols):
                cs, ce = col_edges[c], col_edges[c + 1]

                prev_block = prev_gray[rs:re, cs:ce]
                curr_block = curr_gray[rs:re, cs:ce]

                # avoid tiny blocks instability if tiny blocks they don't count
                if curr_block.size < 16:
                    pass
                else:
                    s += float(global_ssim(curr_block, prev_block))
                n += 1

        return float(s / max(n, 1))


============================================================
========================= FILE: src/boxing_project/shout_boundary/inference.py =========================
============================================================

from __future__ import annotations

from dataclasses import dataclass
import numpy as np

from .detector import ShotBoundaryDetector, ShotBoundaryConfig


@dataclass(frozen=True)
class ShotBoundaryInferConfig:
    """Конфіг для stateful shot-boundary інференсу (SSIM + EMA)."""
    resize_w: int = 160
    resize_h: int = 90
    grid_x: int = 4
    grid_y: int = 4
    ema_alpha: float = 0.2


class ShotBoundaryInferencer:
    """Stateful shot-boundary: update(frame) -> g (0..1), де g близько 0 означає сильний cut."""

    def __init__(self, cfg: ShotBoundaryInferConfig):
        """Створює внутрішній ShotBoundaryDetector і тримає prev_frame/EMA між кадрами."""
        sb_cfg = ShotBoundaryConfig(
            resize=(cfg.resize_w, cfg.resize_h),
            grid=(cfg.grid_x, cfg.grid_y),
            ema_alpha=cfg.ema_alpha,
        )
        self.detector = ShotBoundaryDetector(cfg=sb_cfg)

    def update(self, frame_bgr: np.ndarray) -> float:
        """
        Оновлює детектор новим кадром.

        Args:
            frame_bgr: поточний кадр (H,W,3) BGR.

        Returns:
            g in [0,1]: довіра до геометрії. Менше g => більше схоже на cut.
        """
        return float(self.detector.update(frame_bgr))


============================================================
========================= FILE: src/boxing_project/shout_boundary/__init__.py =========================
============================================================

from .detector import ShotBoundaryDetector, ShotBoundaryConfig


============================================================
========================= FILE: src/boxing_project/shout_boundary/ssim_detector.py =========================
============================================================

import numpy as np


def global_ssim(x: np.ndarray, y: np.ndarray, c1: float = 0.01**2, c2: float = 0.03**2) -> float:
    """
    Global SSIM for two grayscale images (float32 in [0,1]), computed over the whole frame.

    x, y: (H, W) float32 in [0,1]

    SSIM = ((2*mu_x*mu_y + c1) * (2*cov_xy + c2)) / ((mu_x^2 + mu_y^2 + c1) * (var_x + var_y + c2))

    Returns float in [-1, 1] but in practice ~[0,1] for images.
    """
    x = x.astype(np.float32)
    y = y.astype(np.float32)

    mu_x = float(x.mean())
    mu_y = float(y.mean())

    dx = x - mu_x
    dy = y - mu_y

    var_x = float((dx * dx).mean())
    var_y = float((dy * dy).mean())
    cov_xy = float((dx * dy).mean())

    num = (2.0 * mu_x * mu_y + c1) * (2.0 * cov_xy + c2)
    den = (mu_x * mu_x + mu_y * mu_y + c1) * (var_x + var_y + c2)

    if den <= 1e-12:
        return 1.0  # practically identical / constant frames

    return float(num / den)


============================================================
========================= FILE: src/boxing_project/tracking/inference_utils.py =========================
============================================================

import os
import sys
import cv2
import numpy as np
from pathlib import Path

"""
This module contains reusable inference components:
- OpenPose initialization
- Image preprocessing
- Keypoint-based bbox extraction
- Frame processing (OpenPose -> tracker -> drawing)
- Visualization loop
"""

op = None  # will be set in init_openpose_from_config()


def init_openpose_from_config(openpose_cfg: dict):
    """
    Initialize OpenPose wrapper from YAML config.
    Returns (op_module, opWrapper).
    """
    global op

    root = os.path.expanduser(openpose_cfg["root"])
    py_path = os.path.join(root, "build", "python")

    if py_path not in sys.path:
        sys.path.append(py_path)

    try:
        from openpose import pyopenpose as op_module
    except Exception as e:
        raise RuntimeError(
            "Failed to import pyopenpose. Check OpenPose installation and the 'root' path."
        ) from e

    op = op_module

    params = dict()
    params["model_folder"] = os.path.join(root, "models")
    params["hand"] = openpose_cfg.get("hand", False)
    params["face"] = openpose_cfg.get("face", False)
    params["net_resolution"] = openpose_cfg.get("net_resolution", "-1x256")
    params["num_gpu"] = openpose_cfg.get("num_gpu", 1)
    params["num_gpu_start"] = openpose_cfg.get("num_gpu_start", 0)
    params["render_pose"] = openpose_cfg.get("render_pose", 0)
    params["disable_blending"] = openpose_cfg.get("disable_blending", True)
    params["number_people_max"] = openpose_cfg.get("number_people_max", 5)
    params["disable_multi_thread"] = openpose_cfg.get("disable_multi_thread", True)

    opWrapper = op.WrapperPython()
    opWrapper.configure(params)
    opWrapper.start()

    return op_module, opWrapper



def preprocess_image(opWrapper, img_path: Path, save_width: int, return_img=False):
    """
    Read and resize an image, run OpenPose forward pass.
    Returns Datum (and optionally the resized image).
    """
    img = cv2.imread(str(img_path))
    if img is None:
        raise RuntimeError(f"Failed to load image: {img_path}")

    h, w = img.shape[:2]
    if w > save_width:
        scale = save_width / w
        img = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)

    datum = op.Datum()
    datum.cvInputData = img

    datums = op.VectorDatum()
    datums.append(datum)

    opWrapper.emplaceAndPop(datums)

    if return_img:
        return datums[0], img
    return datums[0]



def keypoints_to_bbox(kps, conf_th=0.1):
    """
    Compute bounding box over keypoints with confidence > conf_th.
    Returns None if no valid keypoints found.
    """
    valid = kps[:, 2] > conf_th
    if not valid.any():
        return None

    xs, ys = kps[valid, 0], kps[valid, 1]
    return int(xs.min()), int(ys.min()), int(xs.max()), int(ys.max())



def _rects_intersect(r1, r2) -> bool:
    x1, y1, x2, y2 = r1
    X1, Y1, X2, Y2 = r2
    if x2 <= X1 or X2 <= x1:
        return False
    if y2 <= Y1 or Y2 <= y1:
        return False
    return True


def _find_label_position(base_x, base_ty,
                         label_width, label_height,
                         img_w,
                         label_rects,
                         step,
                         min_y):
    """
    Підбирає вертикальну позицію для тексту так, щоб
    його прямокутник не перетинався з уже існуючими в label_rects.
    Повертає (x, baseline_y) і ДОПИСУЄ прямокутник у label_rects.
    """
    # щоб не вилізти за праву межу кадру
    base_x = max(0, min(base_x, img_w - label_width - 1))

    # щоб текст не вилазив за верх і був не нижче min_y
    ty = max(min_y, base_ty)

    while True:
        top = ty - label_height
        bottom = ty
        rect = (base_x, top, base_x + label_width, bottom)

        if top < 0:
            # далі піднімати вже нікуди
            break

        conflict = any(_rects_intersect(rect, r) for r in label_rects)
        if not conflict:
            break

        # піднімаємо текст вище
        ty -= step

    # запам'ятовуємо зайняту зону
    label_rects.append((base_x, ty - label_height, base_x + label_width, ty))
    return base_x, ty


def process_frame(result, tracker, original_img, conf_th):
    """
    Convert OpenPose output to tracker input, update tracker and draw results.
    Returns processed_frame, log_dict.
    """
    frame = original_img.copy()
    h, w = frame.shape[:2]

    if result.poseKeypoints is None:
        return frame, {
            "matches": [],
            "cost_matrix": np.zeros((0, 0)),
            "active_tracks": [],
        }

    kps = result.poseKeypoints
    people = [{"keypoints": kps[i]} for i in range(len(kps))]

    log = tracker.update_with_openpose(people)

    # Precompute all bounding boxes
    bboxes = [keypoints_to_bbox(kps[i], conf_th) for i in range(len(kps))]

    # --------- налаштування для всіх текстових підписів (ID + OP) ---------
    label_rects = []          # тут лежать прямокутники всіх підписів
    label_height = 18
    label_width_est_id = 60   # приблизна ширина "ID 123"
    label_width_est_op = 70   # приблизна ширина "OP 12"
    step = label_height + 4   # крок підняття
    min_y = label_height + 2  # щоб текст не вилазив за верх
    # ----------------------------------------------------------------------

    # ---- Draw tracks: bbox + ID (з урахуванням неперекриття тексту) ----
    for track_id, det_idx in log["matches"]:
        bb = bboxes[det_idx]
        if bb is None:
            continue

        x1, y1, x2, y2 = bb

        # обмежимо bbox рамками зображення
        x1 = max(0, min(x1, w - 1))
        x2 = max(0, min(x2, w))
        y1 = max(0, min(y1, h - 1))
        y2 = max(0, min(y2, h))

        if x2 <= x1 or y2 <= y1:
            continue

        # сам bbox
        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 255, 255), 2)

        # шукаємо місце для "ID N"
        base_x_id = x1
        base_ty_id = y1 - 5  # базовий baseline над bbox

        x_text_id, ty_id = _find_label_position(
            base_x=base_x_id,
            base_ty=base_ty_id,
            label_width=label_width_est_id,
            label_height=label_height,
            img_w=w,
            label_rects=label_rects,
            step=step,
            min_y=min_y,
        )

        cv2.putText(
            frame,
            f"ID {track_id}",
            (x_text_id, ty_id),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (36, 255, 12),
            2,
            cv2.LINE_AA,
        )

        # лінія від тексту до верхнього лівого кута bbox
        cv2.line(
            frame,
            (x_text_id, ty_id - label_height // 2),
            (x1, y1),
            (255, 255, 255),
            1,
            cv2.LINE_AA,
        )

    # ---- Draw OpenPose det indices: "OP i" теж без перекриття ----
    for det_idx, bb in enumerate(bboxes):
        if bb is None:
            continue
        x1, y1, _, _ = bb

        base_x_op = x1
        base_ty_op = y1 - 25  # хочемо, щоб "OP" був трохи вище, ніж ID

        x_text_op, ty_op = _find_label_position(
            base_x=base_x_op,
            base_ty=base_ty_op,
            label_width=label_width_est_op,
            label_height=label_height,
            img_w=w,
            label_rects=label_rects,
            step=step,
            min_y=min_y,
        )

        cv2.putText(
            frame,
            f"OP {det_idx}",
            (x_text_op, ty_op),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (0, 255, 255),
            2,
        )

    return frame, log




def visualize_sequence(opWrapper, tracker, images, save_width, merge_n):
    """
    Iterate through images, run inference, print debug info, and visualize batches.
    """
    frames = []
    count = 0

    # Resolve debug level
    show_level = getattr(tracker, "show_level", 1)

    from src.boxing_project.tracking.tracking_debug import (
        print_pre_tracking_results,
        print_tracking_results,
    )

    for idx, path in enumerate(images):
        # use 1-based index for logging: 1, 2, 3, ...
        frame_idx = idx + 1

        if show_level >= 2:
            print_pre_tracking_results(frame_idx)

        result, img = preprocess_image(opWrapper, path, save_width, return_img=True)
        frame, log = process_frame(result, tracker, img, tracker.cfg.min_kp_conf)

        # ---- draw "Frame N" label in the top-right corner ----
        h, w = frame.shape[:2]
        text = f"Frame {frame_idx}"
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        color = (0, 255, 0)
        thickness = 2

        text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)
        text_w, text_h = text_size
        org = (w - text_w - 10, text_h + 10)

        cv2.putText(frame, text, org, font, font_scale, color, thickness, cv2.LINE_AA)
        # ------------------------------------------------------

        if show_level >= 1:
            print_tracking_results(log, frame_idx)

        frames.append(frame)
        count += 1

        if count == merge_n and show_level >= 1:
            _show_merged(frames, merge_n)
            frames = []
            count = 0




def _show_merged(frames, n):
    """
    Merge multiple frames horizontally (aligning by height) and show via cv2.imshow.
    """
    max_h = max(f.shape[0] for f in frames)

    aligned = []
    for f in frames:
        h, w = f.shape[:2]
        if h < max_h:
            top = (max_h - h) // 2
            bottom = max_h - h - top
            f = cv2.copyMakeBorder(f, top, bottom, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))
        aligned.append(f)

    combined = cv2.hconcat(aligned)

    cv2.imshow(f"Tracking ({n} frames)", combined)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


============================================================
========================= FILE: src/boxing_project/tracking/__init__.py =========================
============================================================

from pathlib import Path


# All tracking files have the same path to  YAML configuration.

DEFAULT_TRACKING_CONFIG_PATH = (
    Path(__file__).resolve().parents[3] / "configs" / "tracking.yaml"
)

__all__ = ["DEFAULT_TRACKING_CONFIG_PATH"]


============================================================
========================= FILE: src/boxing_project/tracking/matcher.py =========================
============================================================

from __future__ import annotations

import copy
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Optional, Callable, Dict, Any, Union

import numpy as np
from scipy.optimize import linear_sum_assignment

from .track import Track, Detection
from . import DEFAULT_TRACKING_CONFIG_PATH
from .tracking_debug import (
    DebugLog,
    create_matcher_log,
    make_pair_base,
    fill_pair_gated_out,
    fill_pair_ok,
    print_gating_result,
    print_pair_result,
    set_pose_no_keypoints,
    set_pose_no_good_points,
    fill_pose_full_debug,
)


# ----------------------------- #
#         Match config          #
# ----------------------------- #

# src/boxing_project/tracking/matcher.py  (у MatchConfig додай поля)

@dataclass
class MatchConfig:
    """"Конфіг для побудови cost matrix (motion/pose/app + gating)." """
    alpha: float
    chi2_gating: float
    large_cost: float
    pose_scale_eps: float
    keypoint_weights: Optional[np.ndarray]
    min_kp_conf: float


    w_motion_base: float = 1.0
    w_pose_base: float = 1.0
    w_pose_cut: float = 2.0
    w_app_base: float = 1.0
    w_app_cut: float = 3.0
    emb_ema_alpha: float = 0.9


# ----------------------------- #
#        Pose helpers           #
# ----------------------------- #


def _pose_distance(
    track: Track,
    det: Detection,
    cfg: MatchConfig,
    log: Optional[DebugLog] = None,
    pair_tag: str = ""
) -> Tuple[float, Dict[str, Any]]:
    """
    Compute pose-based distance between a track and a detection.

    Assumptions in this simplified version:
      - Input keypoints are already normalized (no centering / scaling here).
      - All joints have the same weight (no per-joint weighting).
      - Confidences are used only to select "good" joints (>= min_kp_conf),
        not as weights in the final average.

    Steps:
      1) Take last keypoints from the track and current keypoints from the detection.
      2) Drop joints with low confidence or NaNs.
      3) Directly compute per-joint Euclidean distance on used joints.
      4) Return simple mean distance and a dict with detailed debug information.

    Returns:
      D_pose : float
          Final pose distance between track and detection.
      pose_dict : Dict[str, Any]
          Detailed information for debugging / logging.
    """
    pose_dict: Dict[str, Any] = {}

    # No keypoints → nothing to compare
    if track.last_keypoints is None or det.keypoints is None:
        set_pose_no_keypoints(pose_dict, log, pair_tag)
        return 0.0, pose_dict

    kpt_t = np.asarray(track.last_keypoints, dtype=float)  # (K, 2)
    kpt_d = np.asarray(det.keypoints, dtype=float)         # (K, 2)
    n_k = kpt_t.shape[0]

    # Confidences for filtering; if missing → ones
    conf_t = (
        np.asarray(track.last_kp_conf, dtype=float).reshape(-1)
        if track.last_kp_conf is not None
        else np.ones((n_k,), dtype=float)
    )
    conf_d = (
        np.asarray(det.kp_conf, dtype=float).reshape(-1)
        if det.kp_conf is not None
        else np.ones((n_k,), dtype=float)
    )

    assert kpt_t.shape[0] == kpt_d.shape[0], "Track/Det must have same number of keypoints"

    # "Good" joints: finite coords + conf >= min_kp_conf on both sides
    good_t = np.isfinite(kpt_t).all(axis=1) & (conf_t >= cfg.min_kp_conf)
    good_d = np.isfinite(kpt_d).all(axis=1) & (conf_d >= cfg.min_kp_conf)
    good = good_t & good_d

    if not np.any(good):
        # No reliable joints for this pair
        set_pose_no_good_points(pose_dict, log, pair_tag, good)
        return 0.0, pose_dict

    # No normalization: we assume kpt_t and kpt_d already in a comparable space
    kpt_t_used = kpt_t[good]
    kpt_d_used = kpt_d[good]

    diff_used = kpt_t_used - kpt_d_used           # (N_good, 2)
    per_k_used = np.linalg.norm(diff_used, axis=1)  # (N_good,)

    # All joints have equal weight → simple mean over used joints
    w_used = np.ones_like(per_k_used, dtype=float)
    D_pose = float(per_k_used.mean())

    # For debug we still call fill_pose_full_debug, passing original coordinates
    fill_pose_full_debug(
        pose_dict=pose_dict,
        log=log,
        pair_tag=pair_tag,
        n_k=n_k,
        good_mask=good,
        kpt_tn=kpt_t,          # not normalized, but used as-is for logging
        kpt_dn=kpt_d,          # same
        diff_used=diff_used,
        per_k_used=per_k_used,
        w_used=w_used,
        D_pose=D_pose,
    )

    return D_pose, pose_dict


def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:
    """
    Cosine distance = 1 - cos(a,b).

    Повертає 1.0, якщо хоч один вектор нульовий/порожній,
    щоб уникнути ділення на 0.
    """
    a = np.asarray(a, dtype=np.float32).reshape(-1)
    b = np.asarray(b, dtype=np.float32).reshape(-1)
    na = float(np.linalg.norm(a))
    nb = float(np.linalg.norm(b))
    if na < 1e-8 or nb < 1e-8:
        return 1.0
    return float(1.0 - float(np.dot(a, b) / (na * nb)))


# ----------------------------- #
#       Motion + gating         #
# ----------------------------- #

def _motion_cost_with_gating(
    track: Track,
    det: Detection,
    cfg: MatchConfig,
    log: Optional[DebugLog] = None,
    pair_tag: str = ""
) -> Tuple[float, bool, float]:
    """
    Compute motion-based cost between a track and a detection using Kalman gating.

    Steps:
      1) Use KalmanTracker.gating_distance() to get Mahalanobis distance^2 (d2)
         between predicted state and detection center.
      2) Compare d2 with chi2_gating threshold:
         - if d2 > chi2_gating -> pair is NOT allowed (pruned).
         - else -> pair is allowed.
      3) Motion cost = sqrt(d2) (for allowed pairs; still defined for logging).

    Returns:
      d_motion : float
          Motion distance (sqrt of d2).
      allowed : bool
          Whether this pair passes the χ² gating.
      d2 : float
          Raw Mahalanobis distance squared (for debug).
    """
    d2 = track.kf.gating_distance(np.asarray(det.center, dtype=float))
    allowed = (d2 <= cfg.chi2_gating)
    d_motion = float(np.sqrt(max(d2, 0.0)))

    if log and log.enabled_print:
        log.section(f"[{pair_tag}] MOTION")
        check = "✓" if allowed else "✗"
        log._print(f"• d² = {d2:.6f}   |   χ²_gate = {cfg.chi2_gating:.6f}   |   allowed = {check}")
        log._print(f"• d_motion = √(d²) = {d_motion:.6f}")

    return d_motion, allowed, float(d2)


# ----------------------------- #
#        Cost matrix C          #
# ----------------------------- #

def build_cost_matrix(
    tracks: List[Track],
    detections: List[Detection],
    cfg: MatchConfig,
    show: bool = True,
    sink: Optional[Callable[[str], None]] = None,
    g: float = 1.0,
) -> Tuple[np.ndarray, Dict[str, Any]]:
    """
    Будує cost matrix для Hungarian.

    Формула:
        cost = w_motion(g) * d_motion
             + w_pose(g)   * d_pose
             + w_app(g)    * d_app

    Де:
      - d_motion: sqrt(Mahalanobis^2) з Kalman gating (через _motion_cost_with_gating)
      - d_pose: cosine distance між track.pose_emb_ema та det.meta["e_pose"]
                fallback: _pose_distance() по keypoints
      - d_app: cosine distance між track.app_emb_ema та det.meta["e_app"]
               якщо нема embedding -> 0.0

    Повертає:
      - C: (n_tracks, n_dets)
      - log_matcher: dict з ключем "pairs" (щоб tracker.py міг зібрати pair logs)
    """
    n_t = len(tracks)
    n_d = len(detections)
    C = np.zeros((n_t, n_d), dtype=np.float32)

    # clamp g
    g = float(max(0.0, min(1.0, g)))

    # ваги
    w_motion = g * float(cfg.w_motion_base)
    w_pose = (1.0 - g) * float(cfg.w_pose_cut) + g * float(cfg.w_pose_base)
    w_app = (1.0 - g) * float(cfg.w_app_cut) + g * float(cfg.w_app_base)

    # Мінімальний лог у форматі, який очікує tracker.py
    log_matcher: Dict[str, Any] = {
        "g": g,
        "weights": {"w_motion": w_motion, "w_pose": w_pose, "w_app": w_app},
        "shape": [n_t, n_d],
        "pairs": []
    }

    # debug log (твій DebugLog), але можна не вмикати
    log = create_matcher_log(cfg, (n_t, n_d), show=show, sink=sink)

    for i, trk in enumerate(tracks):
        for j, det in enumerate(detections):
            pair_tag = f"{i}-{j}"

            # motion + gating
            d_motion, allowed, d2 = _motion_cost_with_gating(trk, det, cfg, log=log, pair_tag=pair_tag)

            pair_obj = make_pair_base(track_index=i, det_index=j)
            if not allowed:
                C[i, j] = float(cfg.large_cost)
                fill_pair_gated_out(
                    pair_obj=pair_obj,
                    cfg=cfg,
                    d2=d2,
                    d_motion=d_motion,
                    cost=float(cfg.large_cost),
                )
                log_matcher["pairs"].append(pair_obj)
                if log.enabled_print:
                    print_gating_result(log, pair_tag, allowed, d2, cfg.chi2_gating, cfg.large_cost)
                continue

            # pose: embedding або fallback keypoints
            d_pose_raw, pose_dict = _pose_distance(trk, det, cfg, log=log, pair_tag=pair_tag)

            if trk.pose_emb_ema is not None and "e_pose" in det.meta:
                d_pose = cosine_distance(trk.pose_emb_ema, det.meta["e_pose"])
                pose_reason = "pose_embedding"
            else:
                d_pose = float(d_pose_raw)
                pose_reason = "pose_keypoints_fallback"

            # appearance: embedding або 0
            if trk.app_emb_ema is not None and "e_app" in det.meta:
                d_app = cosine_distance(trk.app_emb_ema, det.meta["e_app"])
                app_reason = "app_embedding"
            else:
                d_app = 0.0
                app_reason = "no_app_embedding"

            # final cost
            cost = w_motion * float(d_motion) + w_pose * float(d_pose) + w_app * float(d_app)
            C[i, j] = float(cost)

            # log pair
            fill_pair_ok(
                pair_obj=pair_obj,
                cfg=cfg,
                d_motion=float(d_motion),
                d_pose=float(d_pose),
                cost=float(cost),
                pose_dict=pose_dict,
            )
            # доповнимо інфо, щоб було видно нові компоненти
            pair_obj["final"]["components"]["d_app"] = float(d_app)
            pair_obj["final"]["components"]["w_motion"] = float(w_motion)
            pair_obj["final"]["components"]["w_pose"] = float(w_pose)
            pair_obj["final"]["components"]["w_app"] = float(w_app)
            pair_obj["final"]["components"]["g"] = float(g)
            pair_obj["final"]["reason"] = f"{pose_reason}+{app_reason}"

            log_matcher["pairs"].append(pair_obj)

            if log.enabled_print:
                print_pair_result(log, pair_tag, pair_obj, d_motion, d_pose, C[i, j])

    return C, log_matcher



# ----------------------------- #
#      Hungarian + unmatched    #
# ----------------------------- #

def linear_assignment_with_unmatched(
    C: np.ndarray,
    large_cost: float
) -> Tuple[List[Tuple[int, int]], List[int], List[int]]:
    """
    Run Hungarian algorithm on cost matrix and also return unmatched indices.

    Steps:
      1) Run linear_sum_assignment on C.
      2) Filter out assignments whose cost >= large_cost (treated as invalid).
      3) Build:
         - matched : list of (track_idx, det_idx) for valid assignments.
         - unmatched_tracks : track indices not present in matched.
         - unmatched_dets   : detection indices not present in matched.

    Parameters:
      C : np.ndarray
          Cost matrix (tracks x detections).
      large_cost : float
          Threshold for "invalid" matches (e.g. gated out or too expensive).

    Returns:
      matched : List[Tuple[int, int]]
      unmatched_tracks : List[int]
      unmatched_dets : List[int]
    """
    if C.size == 0:
        return [], list(range(C.shape[0])), list(range(C.shape[1]))

    rows, cols = linear_sum_assignment(C)

    matched: List[Tuple[int, int]] = []
    used_tracks = set()
    used_dets = set()

    for r, c in zip(rows, cols):
        if C[r, c] >= large_cost:
            # This match is effectively forbidden (gated out or too costly).
            continue
        matched.append((int(r), int(c)))
        used_tracks.add(int(r))
        used_dets.add(int(c))

    n_tracks, n_dets = C.shape
    unmatched_tracks = [i for i in range(n_tracks) if i not in used_tracks]
    unmatched_dets = [j for j in range(n_dets) if j not in used_dets]

    return matched, unmatched_tracks, unmatched_dets


# ----------------------------- #
#        Config loading         #
# ----------------------------- #

def _load_match_config_from_yaml(
    config_path: Optional[Union[str, Path]] = None,
) -> MatchConfig:
    """
    Read MatchConfig from a YAML file via utils.config helpers.

    If config_path is None, DEFAULT_TRACKING_CONFIG_PATH is used.
    """
    from src.boxing_project.utils.config import load_tracking_config

    resolved = Path(config_path) if config_path is not None else DEFAULT_TRACKING_CONFIG_PATH
    # load_tracking_config already creates the dataclass instances.
    _, match_cfg, _ = load_tracking_config(str(resolved))
    # Copy to avoid sharing mutable state across callers.
    return copy.deepcopy(match_cfg)


# ----------------------------- #
#    Public matching function   #
# ----------------------------- #

def match_tracks_and_detections(
        tracks,
        detections,
        cfg=None,
        debug=True,
        sink=None,
        config_path=None,
        g: float = 1.0,
):
    """
    High-level API: match existing tracks with current detections.

    If cfg is None:
      - Load MatchConfig from YAML (config_path or default).

    Then:
      1) Build cost matrix C via build_cost_matrix().
      2) Run Hungarian matching with linear_assignment_with_unmatched().
      3) Return matches and debug information.

    Returns:
      matches : List[Tuple[int, int]]
          (track_index, detection_index) pairs.
      um_tr   : List[int]
          Indices of unmatched tracks.
      um_dt   : List[int]
          Indices of unmatched detections.
      C       : np.ndarray
          Final cost matrix used for Hungarian.
      log_matcher : Dict[str, Any]
          Detailed matcher log for this frame (for debugging / visualization).

     g ∈ [0,1] — довіра до motion:
    - g=1: motion домінує (звичайні кадри)
    - g=0: motion гаситься, embeddings домінують (camera cut)
    """
    if cfg is None:
        cfg = _load_match_config_from_yaml(config_path)

    C, log_matcher = build_cost_matrix(
        tracks=tracks,
        detections=detections,
        cfg=cfg,
        show=debug,
        sink=sink,
        g=g
    )
    matches, um_tr, um_dt = linear_assignment_with_unmatched(C, cfg.large_cost)
    return matches, um_tr, um_dt, C, log_matcher


============================================================
========================= FILE: src/boxing_project/tracking/tracker.py =========================
============================================================

from __future__ import annotations
import copy
import numbers
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional, Union
from collections import defaultdict
import numpy as np

from boxing_project.kalman_filter.kalman import KalmanTracker
from .track import Track, Detection
from .matcher import MatchConfig, match_tracks_and_detections
from . import DEFAULT_TRACKING_CONFIG_PATH


# ----------------------------- #
#          Tracker config       #
# ----------------------------- #

@dataclass
class TrackerConfig:
    """
    Configuration bundle for MultiObjectTracker.

    Attributes:
      dt : float
          Time step between frames (1 / FPS).
      process_var : float
          Process noise variance for Kalman (acceleration).
      measure_var : float
          Measurement noise variance for Kalman.
      p0 : float
          Initial covariance scaling for Kalman.
      max_age : int
          How many frames a track can stay without updates before removal.
      min_hits : int
          Number of updates needed to mark a track as confirmed.
      match : MatchConfig
          Matching configuration (alpha, gating, etc.).
      min_kp_conf : float
          Minimum confidence for a keypoint to be considered in matching.
      expect_body25 : bool
          Hint about expected OpenPose format (BODY_25).
    """
    dt: float
    process_var: float
    measure_var: float
    p0: float
    max_age: int
    min_hits: int
    match: MatchConfig
    min_kp_conf: float
    expect_body25: bool


# ----------------------------- #
#     Adapter OpenPose → Det    #
# ----------------------------- #

def openpose_people_to_detections(
    people: List[Dict[str, Any]],
    min_kp_conf: float = 0.05,
    expect_body25: bool = True
) -> List[Detection]:
    """
    Converts a list of people from OpenPose JSON into a list of Detection objects.
    Supports several formats:
      1) 'pose_keypoints_2d' — flattened list of length 75 (BODY_25: 25*(x,y,conf))
      2) 'keypoints'/'pose' as np.ndarray or list with shape (K, 3)
      3) 'pose_2d' with fields 'x', 'y', 'conf' (any form that can be reshaped to (K,3))

    Detection center = median of visible (x,y) with conf >= min_kp_conf (robust to outliers).
    """
    dets: List[Detection] = []

    for person in people:
        kps: Optional[np.ndarray] = None

        # option 1: standard OpenPose JSON
        if 'pose_keypoints_2d' in person and isinstance(person['pose_keypoints_2d'], (list, tuple)):
            arr = np.asarray(person['pose_keypoints_2d'], dtype=float).reshape(-1)
            if arr.size % 3 != 0:
                # incorrect size — skip this person
                continue
            K = arr.size // 3
            # if BODY_25 is expected, K should be 25; but allow others to avoid crashing
            kps = arr.reshape(K, 3)

        # option 2: already provided as (K,3) or (K,2)
        elif 'keypoints' in person:
            arr = np.asarray(person['keypoints'], dtype=float)
            if arr.ndim == 2 and arr.shape[1] >= 2:
                if arr.shape[1] == 2:
                    # no confidence -> add conf=1
                    ones = np.ones((arr.shape[0], 1), dtype=float)
                    arr = np.concatenate([arr, ones], axis=1)
                kps = arr[:, :3]

        elif 'pose' in person:
            arr = np.asarray(person['pose'], dtype=float)
            if arr.ndim == 2 and arr.shape[1] >= 2:
                if arr.shape[1] == 2:
                    ones = np.ones((arr.shape[0], 1), dtype=float)
                    arr = np.concatenate([arr, ones], axis=1)
                kps = arr[:, :3]

        elif 'pose_2d' in person:
            p = person['pose_2d']
            xs = np.asarray(p.get('x', []), dtype=float).reshape(-1, 1)
            ys = np.asarray(p.get('y', []), dtype=float).reshape(-1, 1)
            cs = np.asarray(p.get('conf', []), dtype=float).reshape(-1, 1)
            if xs.shape == ys.shape == cs.shape and xs.size > 0:
                kps = np.concatenate([xs, ys, cs], axis=1)

        if kps is None:
            # no keypoints — skip
            continue

        # Filter by conf: hide poor ones as NaN (so they don't affect calculations)
        good = kps[:, 2] >= float(min_kp_conf)
        xy = kps[:, :2].copy()
        xy[~good] = np.nan

        # Center — median of visible points (robust against outliers)
        if np.all(~good):
            # if no reliable points — skip this person
            continue
        cx = np.nanmedian(xy[:, 0])
        cy = np.nanmedian(xy[:, 1])

        dets.append(
            Detection(
                center=(float(cx), float(cy)),
                keypoints=xy,      # (K, 2) with NaN where conf is low
                kp_conf=kps[:, 2], # (K,)
                meta={'raw': person}
            )
        )

    return dets


# ----------------------------- #
#            TRACKER            #
# ----------------------------- #

@lru_cache(maxsize=None)
def _cached_tracking_config(path: str):
    """Cache loader so multiple trackers reuse the parsed YAML."""
    from src.boxing_project.utils.config import load_tracking_config
    return load_tracking_config(path)


def _load_tracker_config_from_yaml(
    config_path: Optional[Union[str, Path]] = None,
) -> Tuple[TrackerConfig, Dict[str, Any]]:
    """Load ``TrackerConfig`` and raw dictionary from YAML file."""

    resolved = Path(config_path) if config_path is not None else DEFAULT_TRACKING_CONFIG_PATH
    tracker_cfg, match_cfg, raw_cfg = _cached_tracking_config(str(resolved))
    tracker_cfg_copy = copy.deepcopy(tracker_cfg)
    # Ensure nested MatchConfig is also unique per tracker instance.
    tracker_cfg_copy.match = copy.deepcopy(match_cfg)
    return tracker_cfg_copy, copy.deepcopy(raw_cfg)


def resolve_show_level(value: Any) -> int:
    """Convert configuration values to the logging level 0/1/2."""

    if isinstance(value, bool):
        return 2 if value else 0

    if value is None:
        return 1

    if isinstance(value, numbers.Integral):
        level = int(value)
    elif isinstance(value, str):
        value = value.strip()
        if value == "":
            return 1
        try:
            level = int(value)
        except ValueError as exc:
            raise ValueError(
                "tracking.show must be an integer 0, 1 or 2"
            ) from exc
    else:
        raise ValueError("tracking.show must be an integer 0, 1 or 2")

    if level not in (0, 1, 2):
        raise ValueError("tracking.show must be one of {0, 1, 2}")

    return level


class MultiObjectTracker:
    """
    Manages a list of tracks:
      predict → build C → Hungarian → update → track lifecycle management.

    If ``cfg`` is not provided, the settings are loaded from the YAML file through
    ``utils.config.load_tracking_config`` (``config_path`` or default
    ``DEFAULT_TRACKING_CONFIG_PATH``).
    """

    def __init__(
        self,
        cfg: Optional[TrackerConfig] = None,
        config_path: Optional[Union[str, Path]] = None,
    ):
        """
        Initialize multi-object tracker.

        You can either:
          - pass a ready TrackerConfig (cfg), or
          - pass a path to YAML (config_path) and let the tracker load it.

        Parameters:
          cfg : TrackerConfig or None
              Pre-built configuration. If provided, config_path must be None.
          config_path : str or Path or None
              Path to YAML with tracking settings. If cfg is None, this is used;
              otherwise ignored.
        """
        if cfg is not None and config_path is not None:
            raise ValueError("Provide either cfg or config_path, not both")

        if cfg is None:
            cfg_loaded, raw_cfg = _load_tracker_config_from_yaml(config_path)
            self.cfg = cfg_loaded
            self._raw_config = raw_cfg
            self.config_path: Optional[Path] = (
                Path(config_path) if config_path is not None else DEFAULT_TRACKING_CONFIG_PATH
            )
        else:
            self.cfg = copy.deepcopy(cfg)
            self._raw_config = None
            self.config_path = Path(config_path) if config_path is not None else None
        self.tracks: List[Track] = []
        self._next_id: int = 1

        raw_cfg = self.get_config_dict() or {}
        show_value = raw_cfg.get("tracking", {}).get("show", None)
        self.show_level: int = resolve_show_level(show_value)
        self.show_debug: bool = self.show_level >= 2

    # ---- utility methods ---- #

    def get_config_dict(self) -> Optional[Dict[str, Any]]:
        """Return a deep copy of the raw YAML configuration used for this tracker."""

        if self._raw_config is None:
            return None
        return copy.deepcopy(self._raw_config)

    def _new_track(self, det: Detection) -> Track:
        """
        Create and initialize a new Track from a detection.

        Steps:
          1) Initialize a KalmanTracker with detection center as the initial state.
          2) Create a Track with a new unique track_id.
          3) Immediately call track.update(det) to align its state with the first measurement.

        Parameters:
          det : Detection
              Detection used to bootstrap the new track.

        Returns:
          Track : newly created and updated Track instance.
        """
        kf = KalmanTracker(
            x0=[det.center[0], det.center[1], 0.0, 0.0],
            dt=self.cfg.dt,
            process_var=self.cfg.process_var,
            measure_var=self.cfg.measure_var,
            p0=self.cfg.p0
        )

        trk = Track(
            track_id=self._next_id,
            kf=kf,
            min_hits=self.cfg.min_hits
        )
        self._next_id += 1
        # immediate first update — to align state with initial measurement
        trk.update(det, ema_alpha=self.cfg.match.emb_ema_alpha)
        return trk

    def _remove_dead(self):
        """
        Remove tracks that exceeded max_age without being updated.

        This keeps the list self.tracks containing only active (alive) tracks.
        """
        self.tracks = [t for t in self.tracks if not t.is_dead(self.cfg.max_age)]

    # ---- per-frame API ---- #

    def update_with_openpose(
        self,
        openpose_people: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Full cycle for a single frame with raw people from OpenPose.

        Returns a dictionary:
          {
            'matches': List[Tuple[track_id, det_index]],
            'unmatched_track_ids': List[int],
            'unmatched_det_indices': List[int],
            'cost_matrix': np.ndarray,
            'active_tracks': List[Dict]  # brief state of tracks after update
          }
        """
        detections = openpose_people_to_detections(
            openpose_people,
            min_kp_conf=self.cfg.min_kp_conf,
            expect_body25=self.cfg.expect_body25
        )
        return self.update(detections)

    def update(
            self,
            detections: List[Detection],
            g: float = 1.0,
    ) -> Dict[str, Any]:
        """
        Main per-frame update step given a list of detections.

        Pipeline:
          1) Predict all existing tracks with Kalman filter.
          2) Build cost matrix and run Hungarian to match tracks ↔ detections.
          3) Update matched tracks with their detections.
          4) Spawn new tracks for unmatched detections.
          5) Remove dead tracks (not updated for > max_age).
          6) Build a summary dict with:
             - matches in (track_id, det_index) space
             - unmatched track ids and detection indices
             - cost matrix
             - per-track debug logs and state snapshot.

        Parameters:
          detections : List[Detection]
              Detections extracted for the current frame.

        Returns:
          Dict[str, Any] : summary of tracking results for this frame.
        """
        # 1) PREDICT
        for trk in self.tracks:
            trk.predict()

        # --- SNAPSHOT: index of track → track_id BEFORE matching ---
        idx2tid = {i: t.track_id for i, t in enumerate(self.tracks)}
        # >>> ADD THIS: list of track_ids in the same order as rows of cost_matrix
        row_track_ids = [idx2tid[i] for i in range(len(self.tracks))]

        # 2) MATCH
        matches_idx, um_tr_idx, um_det_idx, C, log_matcher = match_tracks_and_detections(
            tracks=self.tracks,
            detections=detections,
            cfg=self.cfg.match,
            debug=self.show_debug,
            g=g
        )

        # 3) collect pair_logs_by_tid ...
        pair_logs_by_tid: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
        for p in log_matcher.get("pairs", []):
            i = p.get("track_index")
            tid = idx2tid.get(i)
            if tid is not None:
                pair_logs_by_tid[tid].append(copy.deepcopy(p))

        # 4) UPDATE assigned tracks ...
        id_pairs: List[Tuple[int, int]] = []
        for i_track, j_det in matches_idx:
            trk = self.tracks[i_track]
            det = detections[j_det]
            trk.update(det, ema_alpha=self.cfg.match.emb_ema_alpha)
            id_pairs.append((trk.track_id, j_det))


        # 5) create new tracks from unmatched detections ...
        for j in um_det_idx:
            new_trk = self._new_track(detections[j])
            self.tracks.append(new_trk)
            pair_logs_by_tid[new_trk.track_id].append({
                "track_index": None,
                "det_index": j,
                "motion": None,
                "pose": None,
                "final": {
                    "alpha": self.cfg.match.alpha,
                    "cost": 0.0,
                    "components": {"d_motion": 0.0, "d_pose": 0.0},
                    "reason": "new_track_from_unmatched_detection"
                }
            })

        # 6) remove dead tracks
        self._remove_dead()

        # 7) collect output
        unmatched_track_ids = [idx2tid[i] for i in um_tr_idx if i in idx2tid]

        active_tracks_summary = [
            {
                "track_id": t.track_id,
                "confirmed": t.confirmed,
                "age": t.age,
                "hits": t.hits,
                "time_since_update": t.time_since_update,
                "state": t.state.tolist(),
                "pos": t.pos(),
                "match_log": pair_logs_by_tid.get(t.track_id, [])
            }
            for t in self.tracks
            if not t.is_dead(self.cfg.max_age)
        ]

        return {
            "matches": id_pairs,
            "unmatched_track_ids": unmatched_track_ids,
            "unmatched_det_indices": um_det_idx,
            "cost_matrix": C,
            "active_tracks": active_tracks_summary,
            "frame_log": log_matcher,
            "row_track_ids": row_track_ids  # ← now defined above
        }

    # ---- utilities ---- #

    def get_active_tracks(
        self,
        confirmed_only: bool = True
    ) -> List[Track]:
        """Return a list of active tracks (by default only confirmed ones)."""
        if confirmed_only:
            return [t for t in self.tracks if t.confirmed and not t.is_dead(self.cfg.max_age)]
        return [t for t in self.tracks if not t.is_dead(self.cfg.max_age)]

    def reset(self):
        """Reset all tracks (new video)."""
        self.tracks.clear()
        self._next_id = 1


============================================================
========================= FILE: src/boxing_project/tracking/tracking_debug.py =========================
============================================================

import numpy as np
from typing import Dict, Any, List, Optional, Callable, Tuple


#this file allow me to make a clear code by moving all print function to here
# so there only functions which help in logging information


class DebugLog:

    id: int = 0
    """
    Collects a structured log (dict) and optionally prints; ALWAYS buffers the lines.
    """
    def __init__(
        self,
        enabled_print: bool = True,
        sink: Optional[Callable[[str], None]] = None,
    ):
        self.enabled_print = enabled_print
        self.sink = sink if sink is not None else print
        self.buffer: List[str] = []          # ← buffer of all lines

        self.store: Dict[str, Any] = {
            "config": {},
            "shape": [0, 0],
            "pairs": [],
            "print_chunks": self.buffer,     # ← we expose the buffer upwards
        }

    # --- printing / buffering ---
    def _emit(self, line: str):
        self.buffer.append(line)
        if self.enabled_print:
            self.sink(line)

    def _print(self, msg: str):
        self._emit(msg)

    def section(self, title: str):
        bar = "—" * max(8, len(title))
        self._emit("")         # empty line before section
        self._emit(title)
        self._emit(bar)

    def table(self, header: str, rows: List[str]):
        if header:
            self._emit(header)
        for r in rows:
            self._emit(r)

    # --- writing into dictionary ---
    def add_pair(self, pair_obj: Dict[str, Any]):
        self.store["pairs"].append(pair_obj)

    def set_meta(self, config: "MatchConfig", shape: Tuple[int, int]):
        self.store["config"] = {
            "alpha": config.alpha,
            "chi2_gating": config.chi2_gating,
            "large_cost": config.large_cost,
            "min_kp_conf": config.min_kp_conf,
            "has_keypoint_weights": config.keypoint_weights is not None,
        }
        self.store["shape"] = [int(shape[0]), int(shape[1])]


# ---------- OUTSIDE THE CLASS: helpers for pose ----------

def set_pose_no_keypoints(
    pose_dict: Dict[str, Any],
    log: Optional[DebugLog],
    pair_tag: str,
) -> None:
    """
    Case: there are no keypoints at all (either in track or in detection).
    Fills pose_dict and (optionally) prints something to the log.
    """
    if log:
        log.section(f"[{pair_tag}] POSE")
        log._print("no pose — D_pose=0.0")

    pose_dict.update({
        "has_pose": False,
        "good_mask": None,
        "trk_norm": None,
        "det_norm": None,
        "diff": None,
        "per_k": None,
        "w_eff": None,
        "used_count": 0,
        "D_pose": 0.0
    })


def set_pose_no_good_points(
    pose_dict: Dict[str, Any],
    log: Optional[DebugLog],
    pair_tag: str,
    good_mask: np.ndarray,
) -> None:
    """
    Case: keypoints exist, but there are ZERO jointly good ones.
    """
    if log:
        log.section(f"[{pair_tag}] POSE")
        log._print("no jointly good keypoints — D_pose=0.0")

    pose_dict.update({
        "has_pose": True,
        "good_mask": good_mask.astype(bool).tolist(),
        "trk_norm": None,
        "det_norm": None,
        "diff": None,
        "per_k": None,
        "w_eff": None,
        "used_count": 0,
        "D_pose": 0.0
    })


def _format_pose_rows(
    k_idx: np.ndarray,
    trk_norm: np.ndarray,
    det_norm: np.ndarray,
    diff: np.ndarray,
    per_k: np.ndarray,
    w_eff: np.ndarray,
    used_mask: np.ndarray
) -> List[str]:
    rows = []
    for i, k in enumerate(k_idx):
        tx, ty = trk_norm[k]
        dx_, dy_ = det_norm[k]
        ddx, ddy = diff[i]
        used = bool(used_mask[k])
        rows.append(
            f"{k:2d} | "
            f"{tx:>7.4f} {ty:>7.4f} | "
            f"{dx_:>7.4f} {dy_:>7.4f} | "
            f"{ddx:>7.4f} {ddy:>7.4f} | "
            f"{per_k[i]:>7.4f} | {w_eff[i]:>7.4f} | {str(used):>5s}"
        )
    return rows


def fill_pose_full_debug(
    pose_dict: Dict[str, Any],
    log: Optional[DebugLog],
    pair_tag: str,
    n_k: int,
    good_mask: np.ndarray,
    kpt_tn: np.ndarray,
    kpt_dn: np.ndarray,
    diff_used: np.ndarray,
    per_k_used: np.ndarray,
    w_used: np.ndarray,
    D_pose: float,
) -> None:
    """
    Case: there are valid keypoints and D_pose has been computed.
    Here:
      - expand diff/per_k/w_eff to full length n_k
      - update pose_dict
      - (optionally) print a nice table via DebugLog
    """
    full_diff = np.full((n_k, 2), np.nan, dtype=float)
    full_perk = np.full((n_k,), np.nan, dtype=float)
    full_w = np.full((n_k,), np.nan, dtype=float)

    used_idx = np.where(good_mask)[0]
    full_diff[used_idx] = diff_used
    full_perk[used_idx] = per_k_used
    full_w[used_idx] = w_used  # For w_eff — also only used ones

    pose_dict.update({
        "has_pose": True,
        "good_mask": good_mask.astype(bool).tolist(),
        "trk_norm": kpt_tn.tolist(),
        "det_norm": kpt_dn.tolist(),
        "diff": full_diff.tolist(),
        "per_k": full_perk.tolist(),
        "w_eff": full_w.tolist(),
        "used_count": int(used_idx.size),
        "D_pose": D_pose
    })

    # PRINT (optional)
    if log and log.enabled_print:
        log.section(f"[{pair_tag}] POSE")
        header = (
            " k |    trk_x    trk_y |    det_x    det_y |      dx       dy |   ||d||  |  w_eff | used\n"
            "---+--------------------+--------------------+------------------+---------+--------+------"
        )
        rows = _format_pose_rows(
            k_idx=np.arange(n_k),
            trk_norm=kpt_tn,
            det_norm=kpt_dn,
            diff=full_diff,
            per_k=full_perk,
            w_eff=full_w,
            used_mask=good_mask
        )
        log.table(header, rows)
        log._print(f"\nD_pose = {D_pose:.6f}  (over {int(used_idx.size)} keypoints)")


def print_pre_tracking_results(id):
    print("\n")
    print("=" * 140)
    print(f"         PRE TRACKING RESULTS: {id+1}")
    print("=" * 140)
    print("\n")
# ---------- PRINT TRACKING RESULTS ----------

def print_tracking_results(log: dict, iteration: int, show_pose_tables: bool = False):
    """
    Nicely formatted output of tracking results for a frame:
      - Summary of active tracks
      - Cost matrix with correct Track IDs (row_track_ids)
      - Pair logs for each active track (motion/pose/final)
      - (optional) full 25-keypoint tables for each pair
    """
    # DEBUG, you can remove this later

    print("\n")
    print("=" * 140)
    print(f"         TRACKING RESULTS: {iteration + 1}")
    print("=" * 140)
    print("\n")

    # --- extract things from per-frame log ---
    active_tracks = log.get("active_tracks", [])
    C = np.array(log.get("cost_matrix", []))
    row_track_ids = log.get("row_track_ids", [])

    # 1) ACTIVE TRACKS — brief
    print(f"\n{'active_tracks':<25}: [{len(active_tracks)} active tracks]")
    for t in active_tracks:
        track_id = t.get('track_id', 'N/A')
        confirmed = t.get('confirmed', False)
        hits = t.get('hits', 0)
        age = t.get('age', 0)
        pos = t.get('pos', (0.0, 0.0))
        x_pos = round(pos[0], 2) if pos else 0.0
        y_pos = round(pos[1], 2) if pos else 0.0
        print(f"  |-> ID {track_id} (H:{hits}/A:{age}): "
              f"CONFIRMED={str(confirmed):<5} | POS=({x_pos}, {y_pos})")

    # 2) COST MATRIX — with correct row IDs
    print()
    if C.size == 0:
        print(f"{'cost_matrix':<25}: Empty (no matches to compare)")
    else:
        rows, cols = C.shape
        print(f"{'cost_matrix':<25}: size={rows}x{cols}")
        print("  | Rows = Track ID (before update), Columns = Detection Index")

        # header
        max_id_len = max((len(str(tid)) for tid in row_track_ids), default=3)
        column_width = 8
        header = "  | " + " " * (max_id_len + 9)
        for j in range(cols):
            header += f"Det {j:^{column_width - 4}}"
        print(header)
        print("  |" + "-" * (len(header) - 3))

        # rows
        for i in range(rows):
            tid = row_track_ids[i] if i < len(row_track_ids) else "N/A"
            row_str = f"  | Track {str(tid):>{max_id_len}}: "
            for j in range(cols):
                row_str += f"{C[i, j]:{column_width}.2f} "
            print(row_str)

    # 3) PER-TRACK PAIR LOGS
    print("\nPAIR LOGS (by track):")
    for t in active_tracks:
        tid = t["track_id"]
        pairs = t.get("match_log", [])
        if not pairs:
            print(f"  - Track {tid}: (no logs)")
            continue

        print(f"  - Track {tid}: {len(pairs)} pair(s)")
        for p in pairs:
            reason = p.get("final", {}).get("reason", "ok")
            cost = p.get("final", {}).get("cost", None)
            d_motion = p.get("final", {}).get("components", {}).get("d_motion", None)
            d_pose = p.get("final", {}).get("components", {}).get("d_pose", None)
            det_j = p.get("det_index", None)

            motion = p.get("motion")
            if motion is not None:
                d2 = motion.get("d2", None)
                allowed = motion.get("allowed", None)
            else:
                d2 = None
                allowed = None

            pose = p.get("pose") or {}
            used_count = pose.get("used_count", 0)
            D_pose = pose.get("D_pose", 0.0)

            # short line per pair
            print(f"     • det={det_j}, reason={reason}, "
                  f"d2={None if d2 is None else round(d2, 4)}, "
                  f"allowed={allowed}, "
                  f"d_motion={None if d_motion is None else round(d_motion, 4)}, "
                  f"used_kps={used_count}, D_pose={round(D_pose, 4)}, "
                  f"final_cost={None if cost is None else round(cost, 4)}")

            # (optional) full table over 25 keypoints
            if show_pose_tables and pose and pose.get("trk_norm") is not None:
                trk_norm = np.array(pose["trk_norm"], dtype=float)
                det_norm = np.array(pose["det_norm"], dtype=float)
                diff = np.array(pose["diff"], dtype=float)
                per_k = np.array(pose["per_k"], dtype=float)
                w_eff = np.array(pose["w_eff"], dtype=float)
                good_mask = np.array(pose["good_mask"], dtype=bool)

                print("       k |    trk_x    trk_y |    det_x    det_y |      dx       dy |   ||d||  |  w_eff | used")
                print("      ---+--------------------+--------------------+------------------+---------+--------+------")
                K = trk_norm.shape[0]
                for k in range(K):
                    tx, ty = trk_norm[k]
                    dx_, dy_ = det_norm[k]
                    ddx, ddy = diff[k]
                    perk = per_k[k]
                    ww = w_eff[k]
                    used = bool(good_mask[k])

                    def fmt(v):
                        return "   nan  " if (v is None or np.isnan(v)) else f"{v:7.4f}"

                    print(f"      {k:2d} | "
                          f"{fmt(tx)} {fmt(ty)} | {fmt(dx_)} {fmt(dy_)} | "
                          f"{fmt(ddx)} {fmt(ddy)} | "
                          f"{'   nan  ' if np.isnan(perk) else f'{perk:7.4f}'} | "
                          f"{'   nan ' if np.isnan(ww) else f'{ww:7.4f}'} | "
                          f"{str(used):>5s}")




# ---------- MATCHER HELPERS ----------

def create_matcher_log(
    cfg: "MatchConfig",
    shape: Tuple[int, int],
    show: bool = True,
    sink: Optional[Callable[[str], None]] = None,
) -> DebugLog:
    log = DebugLog(enabled_print=show, sink=sink)
    log.set_meta(cfg, shape)
    return log


def make_pair_base(track_index: int, det_index: int) -> Dict[str, Any]:
    return {
        "track_index": track_index,
        "det_index": det_index,
    }


def fill_pair_gated_out(
    pair_obj: Dict[str, Any],
    cfg: "MatchConfig",
    d_motion: float,
) -> None:
    pair_obj["pose"] = {
        "has_pose": False,
        "good_mask": None,
        "trk_norm": None,
        "det_norm": None,
        "diff": None,
        "per_k": None,
        "w_eff": None,
        "used_count": 0,
        "D_pose": 0.0,
    }
    pair_obj["final"] = {
        "alpha": cfg.alpha,
        "cost": float(cfg.large_cost),
        "components": {"d_motion": float(d_motion), "d_pose": 0.0},
        "reason": "gated_out",
    }


def fill_pair_ok(
    pair_obj: Dict[str, Any],
    cfg: "MatchConfig",
    d_motion: float,
    d_pose: float,
    cost: float,
    pose_dict: Dict[str, Any],
) -> None:
    pair_obj["pose"] = pose_dict
    pair_obj["final"] = {
        "alpha": cfg.alpha,
        "cost": float(cost),
        "components": {
            "d_motion": float(d_motion),
            "d_pose": float(d_pose),
        },
        "reason": "ok",
    }


def print_gating_result(
    log: Optional[DebugLog],
    pair_tag: str,
    d2: float,
    cfg: "MatchConfig",
) -> None:
    if not (log and log.enabled_print):
        return

    log.section(f"[pair {pair_tag}] RESULT")
    log._print(
        f"Gating rejected the pair (d2={d2:.6f} > {cfg.chi2_gating:.6f}). "
        f"Cost = LARGE_COST={cfg.large_cost:g}"
    )


def print_pair_result(
    log: Optional[DebugLog],
    pair_tag: str,
    cfg: "MatchConfig",
    d_motion: float,
    d_pose: float,
    cost: float,
) -> None:
    if not (log and log.enabled_print):
        return

    alpha = cfg.alpha
    log.section(f"[pair {pair_tag}] RESULT")
    log._print(f"α = {alpha:.4f}")
    log._print(
        "final = α·d_motion + (1-α)·D_pose = "
        f"{alpha:.4f}·{d_motion:.6f} + {(1.0 - alpha):.4f}·{d_pose:.6f} = {cost:.6f}"
    )


============================================================
========================= FILE: src/boxing_project/tracking/track.py =========================
============================================================

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, Tuple
import numpy as np

from boxing_project.kalman_filter.kalman import KalmanTracker


@dataclass
class Detection:
    """
    Lightweight container for a single detection on a frame.

    Attributes:
      center : (x, y) center of the person (e.g. median of visible keypoints).
      keypoints : (K, 2) array with 2D keypoints; may contain NaNs for low-conf joints.
      kp_conf : (K,) array of keypoint confidences.
      meta : arbitrary metadata (e.g. raw OpenPose JSON, bbox, etc.).
    """
    center: Tuple[float, float]
    keypoints: Optional[np.ndarray] = None
    kp_conf: Optional[np.ndarray] = None
    meta: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Track:
    """
    Single target track managed by the tracker.

    Attributes:
      track_id : unique integer ID of this track.
      kf : KalmanTracker instance for motion model.
      min_hits : how many successful updates are needed to mark track as confirmed.
      age : number of frames since creation.
      hits : number of frames where this track was updated with a detection.
      time_since_update : how many frames passed since last update().
      confirmed : whether track is considered reliable (hits >= min_hits).
      last_keypoints : last used keypoints (K, 2) for pose matching.
      last_kp_conf : last keypoint confidences (K,).
      last_det_center : last measurement center used for update.
    """
    track_id: int
    kf: KalmanTracker
    min_hits: int

    age: int = 0
    hits: int = 0
    time_since_update: int = 0
    confirmed: bool = False

    last_keypoints: Optional[np.ndarray] = None
    last_kp_conf: Optional[np.ndarray] = None
    last_det_center: Optional[Tuple[float, float]] = None

    pose_emb_ema: Optional[np.ndarray] = None
    app_emb_ema: Optional[np.ndarray] = None

    def predict(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Advance this track one time step with the Kalman filter.

        Increments:
          - age (total lifetime in frames),
          - time_since_update (frames since last measurement).

        Returns:
          state : np.ndarray (4,) current predicted state [x, y, vx, vy].
          cov   : np.ndarray (4, 4) state covariance matrix.
        """
        self.age += 1
        self.time_since_update += 1
        return self.kf.predict()

    def update(self, det: Detection, ema_alpha: float = 0.9):
        """
        Оновлює трек за matched детекцією.

        Робить:
          1) Kalman update по det.center
          2) оновлює last_* поля (keypoints/conf/center)
          3) EMA-оновлення embeddings (pose/app) з det.meta["e_pose"]/["e_app"]

        Args:
            det: Detection, який Hungarian приписав цьому треку.
            ema_alpha: коеф. EMA (чим ближче до 1, тим повільніше змінюється памʼять треку).
        """
        state, cov = self.kf.update(np.asarray(det.center, dtype=float))
        self.time_since_update = 0
        self.hits += 1
        if not self.confirmed and self.hits >= self.min_hits:
            self.confirmed = True

        self.last_det_center = det.center
        self.last_keypoints = None if det.keypoints is None else np.asarray(det.keypoints, dtype=float)
        self.last_kp_conf = None if det.kp_conf is None else np.asarray(det.kp_conf, dtype=float)

        e_pose = det.meta.get("e_pose", None)
        if e_pose is not None:
            e_pose = np.asarray(e_pose, dtype=np.float32)
            if self.pose_emb_ema is None:
                self.pose_emb_ema = e_pose
            else:
                self.pose_emb_ema = ema_alpha * self.pose_emb_ema + (1.0 - ema_alpha) * e_pose

        e_app = det.meta.get("e_app", None)
        if e_app is not None:
            e_app = np.asarray(e_app, dtype=np.float32)
            if self.app_emb_ema is None:
                self.app_emb_ema = e_app
            else:
                self.app_emb_ema = ema_alpha * self.app_emb_ema + (1.0 - ema_alpha) * e_app

        return state, cov

    def marked_missed(self):
        """
        Placeholder hook for marking a track as 'missed' (unmatched on this frame).

        Currently does nothing, but can be extended in the future if additional
        bookkeeping is needed when a track is not updated by any detection.
        """
        return

    def is_dead(self, max_age: int) -> bool:
        """
        Decide whether this track should be removed.

        A track is considered 'dead' if it has not been updated for more than
        max_age frames.

        Parameters:
          max_age : int
              Maximum allowed time_since_update before removal.

        Returns:
          bool : True if the track should be removed.
        """
        return self.time_since_update > max_age

    @property
    def state(self) -> np.ndarray:
        """
        Current state vector of the Kalman filter.

        Returns:
          np.ndarray (4,) : [x, y, vx, vy].
        """
        return self.kf.get_state()

    def pos(self) -> Tuple[float, float]:
        """
        Convenience method: extract only (x, y) position from the current state.

        Returns:
          (x, y) as floats.
        """
        x, y, *_ = self.state
        return float(x), float(y)

    def project_measurement(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Project the current state into measurement space.

        Useful for debugging and gating:
          z_hat = H x
          S     = H P H^T + R

        Returns:
          z_hat : np.ndarray (2, 1)
              Predicted measurement [x, y].
          S : np.ndarray (2, 2)
              Innovation covariance matrix.
        """
        return self.kf.project()


============================================================
========================= FILE: src/boxing_project/utils/config.py =========================
============================================================

import yaml, random, numpy as np
import tensorflow as tf

def load_cfg(path: str) -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

def set_seed(seed: int):
    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)


def _get(d: dict, path: str, default=None):

    cur = d
    for part in path.split('.'):
        if not isinstance(cur, dict) or part not in cur:
            return default
        cur = cur[part]
    return cur


from boxing_project.tracking.matcher import MatchConfig
from boxing_project.tracking.tracker import TrackerConfig



def make_match_config(cfg: dict) -> MatchConfig:
    alpha = float(_get(cfg, "tracking.matching.alpha", 0.8))
    chi2_gating = float(_get(cfg, "tracking.matching.chi2_gating", 9.21))
    large_cost = float(_get(cfg, "tracking.matching.large_cost", 1e6))
    min_kp_conf = float(_get(cfg, "tracking.matching.min_kp_conf", 0.05))
    keypoint_weights = _get(cfg, "tracking.matching.keypoint_weights", None)

    pose_scale_eps = float(_get(cfg, "tracking.matching.pose_scale_eps", 1e-6))

    if keypoint_weights is not None and not isinstance(keypoint_weights, (list, tuple)):
        raise ValueError("keypoint_weights має бути списком чисел або None")


    return MatchConfig(
        alpha=alpha,
        chi2_gating=chi2_gating,
        large_cost=large_cost,
        min_kp_conf=min_kp_conf,
        keypoint_weights=keypoint_weights,
        pose_scale_eps=pose_scale_eps
    )

def make_tracker_config(cfg: dict, match_cfg: MatchConfig) -> TrackerConfig:
    fps = _get(cfg, "tracking.fps", None)

    if fps is not None:
        dt = 1.0 / float(fps)
    else:
        dt = float(_get(cfg, "tracking.kalman.dt", 1.0 / 60.0))

    process_var = float(_get(cfg, "tracking.kalman.process_var", 3.0))
    measure_var = float(_get(cfg, "tracking.kalman.measure_var", 9.0))
    p0 = float(_get(cfg, "tracking.kalman.p0", 1000.0))


    max_age = int(_get(cfg, "tracking.tracker.max_age", 10))
    min_hits = int(_get(cfg, "tracking.tracker.min_hits", 3))
    min_kp_conf = float(_get(cfg, "tracking.tracker.min_kp_conf", 0.05))
    expect_body25 = bool(_get(cfg, "tracking.tracker.expect_body25", True))

    return TrackerConfig(
        dt=dt,
        process_var=process_var,
        measure_var=measure_var,
        p0=p0,
        max_age=max_age,
        min_hits=min_hits,
        match=match_cfg,
        min_kp_conf=min_kp_conf,
        expect_body25=expect_body25,
    )


def load_tracking_config(path: str):
    cfg = load_cfg(path)
    match_cfg = make_match_config(cfg)
    tracker_cfg = make_tracker_config(cfg, match_cfg)
    return tracker_cfg, match_cfg, cfg




============================================================
========================= FILE: src/boxing_project/utils/__init__.py =========================
============================================================



